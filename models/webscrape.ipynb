{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e87c65fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---Company https://bathcanalcraft.co.uk Summary ---\n",
      "\n",
      "Website: https://bathcanalcraft.co.uk\n",
      "\n",
      "Summary: From the initial concept to the maiden voyage, we prioritise your vision every step of the way. Are you looking for a beautifully formed narrowboat, uniquely designed to fit your lifestyle? Join us in redefining the future of narrowboat cruising, where tradition meets innovation in harmony with the environment.\n",
      "\n",
      "Emails: ['bathcanalcraft@icloud.com']\n",
      "Phones: ['+44 7538 784 613']\n",
      "Address: Contact - Bath Canal Craft ‍ ‍ Home Boats All Boats NB ‘Alica-Lee‘ (in planning) NB ‘Fram‘ NB ‘Silly Knot To‘ NB ‘Slapdash Lilly‘ NB ‘The Doran‘ (sailaway) NB ‘Elaine‘ NB ‘Velore‘ Contact If you’d like to discuss a personalised narrowboat build then please feel free call, email, or come visit… +44 7538 784 613 bathcanalcraft@icloud.com Bath Canal Craft Ltd. Deverill Storage Longbridge Deverill Warminster Wiltshire BA12 7FB Bath Canal Craft Ltd. | All Rights Reserved © 2025\n",
      "\n",
      "---Company https://www.kit.edu/ Summary ---\n",
      "\n",
      "Website: https://www.kit.edu/\n",
      "\n",
      "Summary: Die Mission des KIT ist Teil der Dachstrategie KIT 2025 . Die Ausstellung „Wer entscheidet denn sowas? Zur Eröffnung lädt das ITAS am Donnerstag, 4.\n",
      "\n",
      "Emails: ['internetredaktion@sts.kit.edu', 'info@kit.edu']\n",
      "Phones: ['266749428', '49 721 608', '+49 721 608-44290']\n",
      "Address: KIT - Das KIT - Impressum Karlsruher Institut für Technologie Navigation überspringen Home Leichte Sprache Gebärdensprache Impressum Datenschutz Barrierefreiheit Sitemap Intranet KIT en suchen suchen Das KIT Start Das KIT Das KIT Profil Start Das KIT Profil Profil Mission Exzellenz Start Das KIT Profil Exzellenz Exzellenz Exzellenzuniversität Exzellenzcluster Chancengleichheit und Diversität Nachhaltigkeit Forschungsuniversität in der Helmholtz-Gemeinschaft Zahlen, Daten, Fakten Start Das KIT Profil Zahlen, Daten, Fakten Zahlen, Daten, Fakten Rankings Organisation Start Das KIT Organisation Organisation Leitung Organe und Gremien Start Das KIT Organisation Organe und Gremien Organe und Gremien Aufsichtsrat Präsidium Bereichsleitung KIT-Senat Konvent Personalvertretungen Verfasste Studierendenschaft Bereiche Start Das KIT Organisation Bereiche Bereiche Institute KIT-Fakultäten HGF-Programme KIT-Zentren Verwaltung und Infrastruktur Projektträger Karlsruhe Menschen Start Das KIT Menschen Menschen Persönlichkeiten Geschichte Spitzenforschende Start Das KIT Menschen Spitzenforschende Spitzenforschende Alexander von Humboldt-Stiftung European Research Council Deutsche Forschungsgemeinschaft Engagement Start Das KIT Engagement Engagement KIT GIVING KIT-Stiftung Sponsoring KIT-Alumni KFG Medien Start Das KIT Medien Medien Presseinformationen Start Das KIT Medien Presseinformationen Presseinformationen PI 2025 Archiv Presseinformationen News Start Das KIT Medien News News News 2025 Themenhighlights Social Media Jahresberichte Publikationen Podcasts Rund um das KIT Start Das KIT Rund um das KIT Rund um das KIT 200 Jahre KIT KIT für Alle Kultur und Sport Campusführungen Veranstaltungen Start Das KIT Rund um das KIT Veranstaltungen Veranstaltungen Veranstaltungskalender Standorte und Anfahrt Start Das KIT Standorte und Anfahrt Standorte und Anfahrt Interaktiver Campusplan Lagepläne Themen Start Themen Themen Energie Mobilität Information Klima und Umwelt Materialien Mensch und Technik Elementar- und Astroteilchenphysik Mathematik in der Anwendung Studium Start Studium Studium Vor dem Studium Start Studium Vor dem Studium Vor dem Studium Studiengänge Bewerbung Rat und Hilfe Leben und Lernen in Karlsruhe Internationale Studierende Im Studium Start Studium Im Studium Im Studium Studentische Einrichtungen Intranet Nach dem Studium Start Studium Nach dem Studium Nach dem Studium Formalitäten Master und Promotion Berufseinstieg In Verbindung bleiben Weiterbildung KIT-Fakultäten Rat und Hilfe Leben und Lernen in Karlsruhe Start Studium Leben und Lernen in Karlsruhe Leben und Lernen in Karlsruhe Leben in Karlsruhe Lernen am KIT Unterwegs in Karlsruhe Forschung Start Forschung Forschung Themen Start Forschung Themen Themen Energie Mobilität Information Klima und Umwelt Materialien Mensch und Technik Elementar- und Astroteilchenphysik Mathematik in der Anwendung Exzellenz Start Forschung Exzellenz Exzellenz Exzellenzuniversität Exzellenzcluster Helmholtz-Programme Forschungsverbünde Start Forschung Forschungsverbünde Forschungsverbünde Sonderforschungsbereiche DFG-Forschungsgruppen Wissenschaftlicher Nachwuchs Start Forschung Wissenschaftlicher Nachwuchs Wissenschaftlicher Nachwuchs Qualifikation zur Professur Start Forschung Wissenschaftlicher Nachwuchs Qualifikation zur Professur Qualifikation zur Professur Young Investigator Group Preparation Program KIT-Nachwuchsgruppe KIT Associate Fellow Tenure-Track-Professur Postdoc-Phase Promotion Start Forschung Wissenschaftlicher Nachwuchs Promotion Promotion Promovieren am KIT Promotionsprogramme Forschungsinfrastrukturen Gute wissenschaftliche Praxis Innovation Start Innovation Innovation Technologietransfer Kooperation Innovation in Forschung und Lehre Gründen Karriere Start Karriere Karriere Wissenschaft Start Karriere Wissenschaft Wissenschaft Professuren Wissenschaftliche Mitarbeitende Wissenschaftlicher Nachwuchs Start Karriere Wissenschaft Wissenschaftlicher Nachwuchs Wissenschaftlicher Nachwuchs Promotion Postdoktorat Qualifikation zur Professur Verwaltung und Infrastruktur Start Karriere Verwaltung und Infrastruktur Verwaltung und Infrastruktur IT Technische Infrastruktur Start Karriere Verwaltung und Infrastruktur Technische Infrastruktur Technische Infrastruktur Trainee technische Infrastruktur Schüler und Studierende Start Karriere Schüler und Studierende Schüler und Studierende Ausbildung und Duales Studium Angebote für Schülerinnen und Schüler Start Karriere Schüler und Studierende Angebote für Schülerinnen und Schüler Angebote für Schülerinnen und Schüler Schülerlabore Studienbotschafterinnen Start Karriere Schüler und Studierende Angebote für Schülerinnen und Schüler Studienbotschafterinnen Studienbotschafterinnen Architektur Bauingenieur-, Geo- und Umweltwissenschaften Chemieingenieurwesen und Verfahrenstechnik Elektrotechnik und Informationstechnik Geistes- und Sozialwissenschaften Mathematik Physik KIT als Arbeitgeber Bewerbungsprozess Stellenangebote Startseite Home Leichte Sprache Gebärdensprache Impressum Datenschutz Barrierefreiheit Sitemap Intranet suchen suchen Startseite Das KIT Karlsruher Institut für Technologie Das KIT Themen Studium Forschung Innovation Karriere Das KIT Profil Organisation Menschen Engagement Medien Rund um das KIT Standorte und Anfahrt Impressum Anbieter Copyright Haftung Verweise auf externe Web-Seiten Datenschutz und IT-Sicherheit Anbieter Diensteanbieter im Sinne von §5 Digitale Dienste Gesetz (DDG) und §18 Abs. 2 Medienstaatsvertrag (MStV): Rechtlicher Sitz: Karlsruher Institut für Technologie Kaiserstraße 12 76131 Karlsruhe Deutschland Tel.: +49 721 608-0 Fax: +49 721 608-44290 E-Mail: info@kit.edu Rechtsform: Körperschaft des öffentlichen Rechts Vertretungsberechtigt: Prof. Dr. Jan S. Hesthaven (Präsident des KIT) Verantwortlich für den Inhalt: Margarete Lehné Leiterin Gesamtkommunikation Stab und Strategie Anfragen zum Inhalt E-Mail: internetredaktion@sts.kit.edu Umsatzsteueridentifikationsnummer: DE266749428 Copyright Für die Internet-Seiten des Karlsruher Instituts für Technologie liegen Copyright und alle weiteren Rechte beim Karlsruher Institut für Technologie, Kaiserstraße 12, 76131 Karlsruhe, Deutschland. Weiterverbreitung, auch in Auszügen, für pädagogische, wissenschaftliche oder private Zwecke ist unter Angabe der Quelle gestattet (sofern nicht anders an der entsprechenden Stelle ausdrücklich angegeben). Eine Verwendung im gewerblichen Bereich bedarf der Genehmigung durch das Karlsruher Institut für Technologie. Ansprechpartner ist die Dienstleistungseinheit Stab und Strategie . Haftung Diese Internetseiten dienen lediglich der Information. Ihr Inhalt wurde mit gebührender Sorgfalt zusammengestellt. Das Karlsruher Institut für Technologie übernimmt aber keine Garantie, weder ausdrücklich noch implizit, für die Art oder Richtigkeit des dargebotenen Materials und übernimmt keine Haftung (einschließlich Haftung für indirekten Verlust oder Gewinn- oder Umsatzverluste) bezüglich des Materials bzw. der Nutzung dieses Materials. Sollten Inhalte von Web-Seiten des Karlsruher Instituts für Technologie gegen geltende Rechtsvorschriften verstoßen, dann bitten wir um umgehende Benachrichtigung. Wir werden die Seite oder den betreffenden Inhalt dann schnellstmöglich entfernen. Verweise auf externe Web-Seiten Die Web-Seiten des Karlsruher Instituts für Technologie enthalten Verweise (Links) zu Informationsangeboten auf Servern, die nicht der Kontrolle und Verantwortlichkeit des Karlsruher Instituts für Technologie unterliegen. Das Karlsruher Institut für Technologie übernimmt keine Verantwortung und keine Garantie für diese Informationen und billigt oder unterstützt diese auch nicht inhaltlich. Datenschutz und IT-Sicherheit Hinweise zum Datenschutz finden Sie in unserer Datenschutzerklärung. Sollten Sie weitere Fragen zu unseren Datenschutz-Standards haben, wenden Sie sich bitte an unseren Datenschutzbeauftragten . Möchten Sie sich über die Konzepte und technische Realisierung unserer IT-Sicherheit informieren, wenden Sie sich bitte an unseren Informationssicherheitsbeauftragten . Feedback Ihr Feedback zu dieser Seite ist uns wichtig * Name * E-Mail Adresse * Ihre Nachricht * Captcha Senden * Pflichtfeld letzte Änderung: 21.08.2025 KIT – Die Forschungsuniversität in der Helmholtz-Gemeinschaft Home Leichte Sprache Gebärdensprache Impressum Datenschutz Barrierefreiheit Sitemap Intranet KIT\n",
      "\n",
      "---Company https://mpob.gov.my/ Summary ---\n",
      "\n",
      "Website: https://mpob.gov.my/\n",
      "\n",
      "Summary: Our mission To enhance the well-being of the Malaysian oil palm industry through excellent research & development and services. Conduct research and development related to the oil palm industry. Function Implement policies and development programmes to ensure viability of the oil palm industry of Malaysia.\n",
      "\n",
      "Emails: ['general@mpob.gov.my']\n",
      "Phones: ['603-89259446', '603-87694400']\n",
      "Address: Malaysian Palm Oil Board – 6, Persiaran Institusi, Bandar Baru Bangi<br>43000 Kajang Selangor, Malaysia Skip to content Malaysian Palm Oil Board 6, Persiaran Institusi, Bandar Baru Bangi 43000 Kajang Selangor, Malaysia CERTIFIED TO ISO 9001:2015 CERT. NO.: QMS 02602 Contact Us Sitemap Menu Corporate Info About Us MPOB Logo Vision & Mission Organisation Chart Clientsâ Charter Top Management Board Members Division Research & Development (R&D) Advanced Biotechnology and Breeding CentreÂ Biology Sustainability Research Division Engineering & Processing Research Division Advanced Oleochemical Technology Division Product Development & Advisory Services Division Services Economics & Industry Development Division Licensing & Enforcement Division Smallholder Extension & Certification Division Information Technology & Corporate Services Division Management, Finance & Development Division eServices Industry Application MyLesen e-LesenPK SIMS e-Kilang e-Peniaga e-Submission e-Registration Online Application PALMOILIS Genom Sawit GanoID Mobile application Info Sawit Sawit Secure Public Access Statistic of Online Services (2025) Dataset Internship MSPO Unit Integriti Perjanjian Perkongsian Ekonomi Malaysia-EFTA (MEEPA) Conferences & Courses Conference / Seminars Nutrition Satellite Symposium in conjunction with PIPOC 2025 ISOPB - Advancing The Next Generation of Oil Palm Planting Materials & Annual General Meeting MPOB International Palm Oil Congress and Exhibition (PIPOC) 2025 International Conference on Oil Palm Plant Protection (ICOPP) 2025 Palm Oil Economic Review and Outlook Seminar R&O 2026 Courses List of Courses Semakan Keputusan Peperiksaan Training Calendar Media News Press Release Speeches Photo Gallery Corporate Video Publication Online Journals Journal of Oil Palm Research Journal (JOPR) Oil Palm Industry Economic Journal (OPIEJ) Bulletin Palm Oil Engineering Bulletin (POEB) Warta Sawit eBook MPOB eBook Red Palm Oil eBook Printed Laporan Tahunan MPOB Books MPOB Publication Catalogue 2025 Advances In Oil Palm Research - Second Edition Contact us MPOB Offices Staff Directory Local Office PORTSIM 25th Anniversary Selamat Menyambut Hari Kebangsaan ke-68 31 Ogos 2025 2025_Perpindahan_1 Slider Malaysian Standard 1.8.2025 Slide 2 ISOPB Banner 2025_Nutrition Satellite 3 2025_ICOPP Banner 1920 x 691 (3) Website R&O 2026 2026_PKPKS_2 MPOB Technology Awards and Recognition chart-line-up Daily CPO Prices MYR 4,337.50 3 September 2025 Smallholder Scheme Tenders & Quotations MARCOP MyGOV KSN KPK Email: general@mpob.gov.my Tel: 603-87694400 Fax: 603-89259446 Facebook X-twitter Youtube Instagram Tiktok Disclaimer | Security Policy | Privacy Policy © 2025 Malaysian Palm Oil Board Type your Message X Welcome to MPOB × Ã Table of Contents Table of Contents Search for: PIPOC 2025 Red Palm Oil eBook Printed Publication SPOTME Number of Online Services Privacy Policy Dasar Privasi\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from urllib.parse import urljoin\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "def scrape_text(url):\n",
    "    \"\"\"Fetch and clean visible text from a webpage.\"\"\"\n",
    "    try:\n",
    "        resp = requests.get(url, headers={\"User-Agent\": \"Mozilla/5.0\"}, timeout=10)\n",
    "        resp.raise_for_status()\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching {url}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "    soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "    for tag in soup([\"script\", \"style\", \"noscript\"]):\n",
    "        tag.decompose()\n",
    "\n",
    "    text = soup.get_text(separator=\" \", strip=True)\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    return text\n",
    "\n",
    "def find_relevant_pages(base_url, keywords):\n",
    "    \"\"\"Find pages by keyword relevance.\"\"\"\n",
    "    try:\n",
    "        resp = requests.get(base_url, headers={\"User-Agent\": \"Mozilla/5.0\"}, timeout=10)\n",
    "        resp.raise_for_status()\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching {base_url}: {e}\")\n",
    "        return [base_url]\n",
    "\n",
    "    soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "    links = [a.get(\"href\") for a in soup.find_all(\"a\", href=True)]\n",
    "\n",
    "    relevant_pages = []\n",
    "    for link in links:\n",
    "        if any(k in link.lower() for k in keywords):\n",
    "            full_link = urljoin(base_url, link)\n",
    "            if full_link not in relevant_pages:\n",
    "                relevant_pages.append(full_link)\n",
    "\n",
    "    return relevant_pages\n",
    "\n",
    "def summarize_text(text, max_sentences=3):\n",
    "    \"\"\"Generate a short, crisp company summary.\"\"\"\n",
    "    sentences = re.split(r'(?<=[.!?]) +', text)\n",
    "    keywords = [\"about\", \"vision\", \"mission\", \"objective\", \"focus\", \"establish\",\n",
    "                \"function\", \"research\", \"development\", \"sustain\", \"industry\"]\n",
    "\n",
    "    scored = []\n",
    "    for s in sentences:\n",
    "        score = sum(k in s.lower() for k in keywords)\n",
    "        if 40 < len(s) < 250:  # avoid junk sentences\n",
    "            scored.append((score, s.strip()))\n",
    "\n",
    "    scored.sort(key=lambda x: (-x[0], len(x[1])))\n",
    "    top_sentences = [s for _, s in scored[:max_sentences]]\n",
    "    return \" \".join(top_sentences)\n",
    "\n",
    "def extract_contacts(text):\n",
    "    \"\"\"Extract emails, phone numbers, and possible addresses.\"\"\"\n",
    "    emails = list(set(re.findall(r\"[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,}\", text)))\n",
    "    phones = list(set(re.findall(r\"(?:\\+?\\d{1,3}[-.\\s]?)?\\(?\\d{2,4}\\)?[-.\\s]?\\d{3,5}[-.\\s]?\\d{3,5}\", text)))\n",
    "\n",
    "    # crude heuristic for address\n",
    "    address_candidates = re.findall(r\"([A-Z][^,]+(?:Street|Strasse|Road|Rd|Avenue|Ave|Lane|Ln|Boulevard|Blvd|Way|Place|Pl|Square|Sq|Drive|Dr|Building|Bldg|No\\.|Kaiserstraße).+)\", text, re.IGNORECASE)\n",
    "    address = address_candidates[0] if address_candidates else None\n",
    "\n",
    "    return {\n",
    "        \"emails\": emails,\n",
    "        \"phones\": phones,\n",
    "        \"address\": address\n",
    "    }\n",
    "\n",
    "def company_profile(base_url):\n",
    "    \"\"\"Scrape summary + contacts separately for better results.\"\"\"\n",
    "    # Pages for summary\n",
    "    summary_pages = [base_url] + find_relevant_pages(base_url, [\"about\", \"vision\", \"mission\", \"who-we-are\", \"company\", \"corporate\"])\n",
    "    \n",
    "    # Pages for contact info\n",
    "    contact_pages = find_relevant_pages(base_url, [\"contact\", \"impressum\", \"imprint\"])\n",
    "    if not contact_pages:\n",
    "        contact_pages = [base_url]  # fallback\n",
    "\n",
    "    # Build summary\n",
    "    summary_text = \"\"\n",
    "    for page in summary_pages[:5]:\n",
    "        summary_text += \" \" + scrape_text(page)\n",
    "    summary = summarize_text(summary_text)\n",
    "\n",
    "    # Extract contact info\n",
    "    contact_text = \"\"\n",
    "    for page in contact_pages[:3]:\n",
    "        contact_text += \" \" + scrape_text(page)\n",
    "    contacts = extract_contacts(contact_text)\n",
    "\n",
    "    return {\n",
    "        \"url\": base_url,\n",
    "        \"summary\": summary,\n",
    "        \"contacts\": contacts\n",
    "    }\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    urls = [\"https://bathcanalcraft.co.uk\",\"https://www.kit.edu/\",\"https://mpob.gov.my/\"] \n",
    "    for url in urls:\n",
    "        print(f\"\\n---Company {url} Summary ---\\n\")\n",
    "        profile = company_profile(url)\n",
    "        print(\"Website:\", profile[\"url\"])\n",
    "        print(\"\\nSummary:\", profile[\"summary\"])\n",
    "        print(\"\\nEmails:\", profile[\"contacts\"][\"emails\"])\n",
    "        print(\"Phones:\", profile[\"contacts\"][\"phones\"])\n",
    "        print(\"Address:\", profile[\"contacts\"][\"address\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c87fdfc8",
   "metadata": {},
   "source": [
    "***Smart summary about company using facebook HuggingFace model.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da3f047",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Scraping https://www.bathcanalcraft.co.uk ---\n",
      "\n",
      "Summary:\n",
      " **https://www.bathcanalcraft.co.uk**: Bath Canal Craft offers a truly bespoke service, working closely with you to ensure you have control over the design decisions that matter most. Join us in redefining the future of narrowboat cruising, where tradition meets innovation in harmony with the environment.\n",
      "\n",
      "--- Scraping https://www.kit.edu/ ---\n",
      "\n",
      "Summary:\n",
      " **https://www.kit.edu/**: KIT - Karlsruher Institut für Technologie Navigation überspringen Home Leichte Sprache Gebärdensprache Impressum Datenschutz Barrierefreiheit Sitemap Intranet KIT en suchen suchen.\n",
      "\n",
      "--- Scraping https://www.mpob.gov.my/ ---\n",
      "\n",
      "Summary:\n",
      " **https://www.mpob.gov.my/**: Malaysian Palm Oil Board – 6, Persiaran Institusi, Bandar Baru Bangi 43000 Kajang Selangor, Malaysia CERTIFIED TO ISO 9001:2015 CERT. NO.: QMS 02602 Contact Us Sitemap Menu Corporate Info About Us MPOB Logo Vision & Mission Organisation Chart Clientsâ Charter Top Management Board Members Division Research & Development (R&D) Advanced Biotechnology and Breeding CentreÂ Biology Sustainability Research Division Engineering & Processing Research Division Advanced Oleochemical Technology Division Product Development & Advisory\n",
      "\n",
      "--- Scraping https://www.nseindia.com/ ---\n",
      "\n",
      "Summary:\n",
      " **https://www.nseindia.com/**: NSE - National Stock Exchange of India Ltd: Live Share/Stock Market News &amp; Updates, Quotes- Nseindia.com Option Chain Market Turnover Listings IPO Circulars Daily Report Holidays Corporates Press Releases Contact Us English 24,715.05 135.45 ( 0.55 %)\n",
      "\n",
      "--- Scraping https://www.research.gla.ac.uk ---\n",
      "\n",
      "Normal request failed: HTTPSConnectionPool(host='www.research.gla.ac.uk', port=443): Max retries exceeded with url: / (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x000001EADD1E0D50>: Failed to resolve 'www.research.gla.ac.uk' ([Errno 11001] getaddrinfo failed)\"))\n",
      "Summary:\n",
      " None\n",
      "\n",
      "--- Scraping https://www.bebob.de ---\n",
      "\n",
      "Normal request failed: HTTPSConnectionPool(host='www.bebob.de', port=443): Max retries exceeded with url: / (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x000001EADDDE3FD0>, 'Connection to www.bebob.de timed out. (connect timeout=10)'))\n",
      "Summary:\n",
      " None\n",
      "\n",
      "--- Scraping https://www.as.gov.qa ---\n",
      "\n",
      "Normal request failed: HTTPSConnectionPool(host='www.as.gov.qa', port=443): Max retries exceeded with url: / (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x000001EADD11B610>: Failed to resolve 'www.as.gov.qa' ([Errno 11001] getaddrinfo failed)\"))\n",
      "Summary:\n",
      " None\n",
      "\n",
      "--- Scraping https://www.airjouletech.com ---\n",
      "\n",
      "Summary:\n",
      " **https://www.airjouletech.com**: AirJoule® - Transformational Cooling & Water Technology. Main Navigation Technology Air In. Water Out. A1000 Solutions Water from Air Water Recovery Cooling Systems Moisture Control Applications Data Centers Company About AirJouLE Our Partners Careers News Investors Contact Home THE POWER OF WATER FROM AIR home-data-center 5 million gallons of water per day goes to COOLING A DATA CENTER.\n",
      "\n",
      "--- Scraping https://www.moser-konstruktion.de ---\n",
      "\n",
      "Normal request failed: HTTPSConnectionPool(host='www.moser-konstruktion.de', port=443): Max retries exceeded with url: / (Caused by SSLError(SSLError(1, '[SSL: SSLV3_ALERT_HANDSHAKE_FAILURE] sslv3 alert handshake failure (_ssl.c:1002)')))\n",
      "Summary:\n",
      " None\n",
      "\n",
      "--- Scraping https://www.ac.sce.ac.il ---\n",
      "\n",
      "Normal request failed: HTTPSConnectionPool(host='www.ac.sce.ac.il', port=443): Max retries exceeded with url: / (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x000001EADD0A9310>: Failed to resolve 'www.ac.sce.ac.il' ([Errno 11001] getaddrinfo failed)\"))\n",
      "Summary:\n",
      " None\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from transformers import pipeline\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.filterwarnings(\"ignore\", message=\".*max_new_tokens.*\")\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "# Optional JS rendering\n",
    "try:\n",
    "    from requests_html import HTMLSession\n",
    "    JS_RENDER_AVAILABLE = True\n",
    "except ImportError:\n",
    "    JS_RENDER_AVAILABLE = False\n",
    "\n",
    "# Load local summarizer\n",
    "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "# summarizer = pipeline(\"summarization\", model=\"t5-small\")\n",
    "# -------------------------------\n",
    "# 1. Scrape Website Text\n",
    "# -------------------------------\n",
    "def scrape_website(url):\n",
    "    text = \"\"\n",
    "    try:\n",
    "        r = requests.get(url, headers={\"User-Agent\": \"Mozilla/5.0\"}, timeout=10)\n",
    "        soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "        for script in soup([\"script\", \"style\", \"noscript\"]):\n",
    "            script.extract()\n",
    "        text = \" \".join(soup.stripped_strings)\n",
    "    except Exception as e:\n",
    "        print(f\"Normal request failed: {e}\")\n",
    "\n",
    "    # Fallback to JS-rendered\n",
    "    if len(text.split()) < 50 and JS_RENDER_AVAILABLE:\n",
    "        try:\n",
    "            session = HTMLSession()\n",
    "            r = session.get(url)\n",
    "            r.html.render(timeout=20)\n",
    "            text = r.html.text\n",
    "        except Exception as e:\n",
    "            print(f\"JS render failed: {e}\")\n",
    "\n",
    "    return text\n",
    "\n",
    "# -------------------------------\n",
    "# 3. Extract Objectives and Services\n",
    "# -------------------------------\n",
    "def extract_objective_service_text(text):\n",
    "    paragraphs = [p.strip() for p in text.split(\"\\n\") if len(p.strip()) > 40]\n",
    "\n",
    "    # Keywords\n",
    "    objective_keywords = [\"mission\", \"objective\", \"goal\", \"vision\", \"purpose\", \"focus\", \"aim\"]\n",
    "    service_keywords = [\"service\", \"product\", \"offer\", \"specialize\", \"provide\", \"solutions\", \"design\", \"build\", \"manufacture\"]\n",
    "\n",
    "    # Extract paragraphs\n",
    "    objective_paragraphs = [p for p in paragraphs if any(k in p.lower() for k in objective_keywords)]\n",
    "    service_paragraphs = [p for p in paragraphs if any(k in p.lower() for k in service_keywords)]\n",
    "\n",
    "    combined = objective_paragraphs + service_paragraphs\n",
    "    if combined:\n",
    "        return \" \".join(combined)\n",
    "    else:\n",
    "        # fallback to first few paragraphs\n",
    "        return \" \".join(paragraphs[:10])\n",
    "\n",
    "# -------------------------------\n",
    "# 4. Generate Crisp Summary\n",
    "# -------------------------------\n",
    "def generate_summary(text, company_name=\"The company\"):\n",
    "    text = text[:2000]  # HuggingFace token limit\n",
    "    input_length = len(text.split())\n",
    "    max_length = min(120, input_length)  # Ensure max_length <= input_length\n",
    "    min_length = min(40, max_length - 1) if max_length > 40 else 10\n",
    "    result = summarizer(text, max_length=max_length, min_length=min_length, do_sample=False)\n",
    "    return f\"**{company_name}**: {result[0]['summary_text']}\"\n",
    "\n",
    "# -------------------------------\n",
    "# 5. Full Company Analyzer\n",
    "# -------------------------------\n",
    "def     analyze_company(url, company_name=\"The company\"):\n",
    "    text = scrape_website(url)\n",
    "    objective_service_text = extract_objective_service_text(text)\n",
    "    if objective_service_text:\n",
    "        summary = generate_summary(objective_service_text, company_name)\n",
    "    else:\n",
    "        summary = \"None\"\n",
    "    \n",
    "    return summary\n",
    "    \n",
    "\n",
    "# -------------------------------\n",
    "# Example Usage\n",
    "# -------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    websites = ['https://www.bathcanalcraft.co.uk',\n",
    "        \"https://www.kit.edu/\", \n",
    "        \"https://www.mpob.gov.my/\",\n",
    "        \"https://www.nseindia.com/\", 'https://www.research.gla.ac.uk', \n",
    "        'https://www.bebob.de', 'https://www.as.gov.qa', 'https://www.airjouletech.com',\n",
    "        'https://www.moser-konstruktion.de', 'https://www.ac.sce.ac.il'\n",
    "    ]\n",
    "\n",
    "    for url in websites:\n",
    "        print(f\"\\n--- Scraping {url} ---\\n\")\n",
    "        summary = analyze_company(url)\n",
    "        print(summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e0e1201e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Bath Canal Craft ---\n",
      "\n",
      "Summary:\n",
      " **Bath Canal Craft**: Bath Canal Craft offers a truly bespoke service, working closely with you to ensure you have control over the design decisions that matter most. Join us in redefining the future of narrowboat cruising, where tradition meets innovation in harmony with the environment.\n",
      "\n",
      "--- Karlsruhe Institute of Technology ---\n",
      "\n",
      "Summary:\n",
      " **Karlsruhe Institute of Technology**: KIT - Karlsruher Institut für Technologie Navigation überspringen Home Leichte Sprache Gebärdensprache Impressum Datenschutz Barrierefreiheit Sitemap Intranet KIT en suchen suchen.\n",
      "\n",
      "--- Malaysian Palm Oil Board ---\n",
      "\n",
      "Summary:\n",
      " **Malaysian Palm Oil Board**: Malaysian Palm Oil Board – 6, Persiaran Institusi, Bandar Baru Bangi 43000 Kajang Selangor, Malaysia CERTIFIED TO ISO 9001:2015 CERT. NO.: QMS 02602 Contact Us Sitemap Menu Corporate Info About Us MPOB Logo Vision & Mission Organisation Chart Clientsâ Charter Top Management Board Members Division Research & Development (R&D) Advanced Biotechnology and Breeding CentreÂ Biology Sustainability Research Division Engineering & Processing Research Division Advanced Oleochemical Technology Division Product Development & Advisory\n",
      "\n",
      "--- NSE India ---\n",
      "\n",
      "Summary:\n",
      " **NSE India**: NSE - National Stock Exchange of India Ltd: Live Share/Stock Market News &amp; Updates, Quotes- Nseindia.com Option Chain Market Turnover Listings IPO Circulars Daily Report Holidays Corporates Press Releases Contact Us English 24,715.05 135.45 ( 0.55 %)\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from transformers import pipeline\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "# Optional JS rendering\n",
    "try:\n",
    "    from requests_html import HTMLSession\n",
    "    JS_RENDER_AVAILABLE = True\n",
    "except ImportError:\n",
    "    JS_RENDER_AVAILABLE = False\n",
    "\n",
    "# -------------------------------\n",
    "# Load summarizer once\n",
    "# -------------------------------\n",
    "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "# summarizer = pipeline(\"summarization\", model=\"t5-small\")\n",
    "# -------------------------------\n",
    "# 1. Scrape Website Text\n",
    "# -------------------------------\n",
    "def scrape_website(url):\n",
    "    \"\"\"Fetch visible text from webpage with headers to avoid 403 errors.\"\"\"\n",
    "    text = \"\"\n",
    "    try:\n",
    "        r = requests.get(url, headers={\"User-Agent\": \"Mozilla/5.0\"}, timeout=10)\n",
    "        r.raise_for_status()\n",
    "        soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "        for tag in soup([\"script\", \"style\", \"noscript\"]):\n",
    "            tag.decompose()\n",
    "        text = \" \".join(soup.stripped_strings)\n",
    "    except Exception as e:\n",
    "        print(f\"Normal request failed for {url}: {e}\")\n",
    "\n",
    "    # Fallback to JS-rendered\n",
    "    if len(text.split()) < 50 and JS_RENDER_AVAILABLE:\n",
    "        try:\n",
    "            session = HTMLSession()\n",
    "            r = session.get(url)\n",
    "            r.html.render(timeout=20)\n",
    "            text = r.html.text\n",
    "        except Exception as e:\n",
    "            print(f\"JS render failed for {url}: {e}\")\n",
    "\n",
    "    return text\n",
    "\n",
    "# -------------------------------\n",
    "# 2. Extract Objectives & Services\n",
    "# -------------------------------\n",
    "def extract_objective_service_text(text):\n",
    "    paragraphs = [p.strip() for p in text.split(\"\\n\") if len(p.strip()) > 40]\n",
    "\n",
    "    objective_keywords = [\"mission\", \"objective\", \"goal\", \"vision\", \"purpose\", \"focus\", \"aim\"]\n",
    "    service_keywords = [\"service\", \"product\", \"offer\", \"specialize\", \"provide\", \"solutions\", \"design\", \"build\", \"manufacture\"]\n",
    "\n",
    "    objective_paragraphs = [p for p in paragraphs if any(k in p.lower() for k in objective_keywords)]\n",
    "    service_paragraphs = [p for p in paragraphs if any(k in p.lower() for k in service_keywords)]\n",
    "\n",
    "    combined = objective_paragraphs + service_paragraphs\n",
    "    if combined:\n",
    "        return \" \".join(combined)\n",
    "    return \" \".join(paragraphs[:10]) if paragraphs else \"\"\n",
    "\n",
    "# -------------------------------\n",
    "# 3. Generate Crisp Summary\n",
    "# -------------------------------\n",
    "def generate_summary(text, company_name=\"The company\"):\n",
    "    text = text[:2000]  # HuggingFace token limit\n",
    "    input_length = len(text.split())\n",
    "    max_length = min(120, input_length)\n",
    "    min_length = min(40, max_length - 1) if max_length > 40 else 10\n",
    "    result = summarizer(text, max_length=max_length, min_length=min_length, do_sample=False)\n",
    "    return f\"**{company_name}**: {result[0]['summary_text']}\"\n",
    "\n",
    "# -------------------------------\n",
    "# 4. Full Company Analyzer\n",
    "# -------------------------------\n",
    "def analyze_company(url, company_name=\"The company\"):\n",
    "    text = scrape_website(url)\n",
    "    objective_service_text = extract_objective_service_text(text)\n",
    "    summary = generate_summary(objective_service_text, company_name) if objective_service_text else \"None\"\n",
    "    return {\n",
    "        \"summary\": summary\n",
    "    }\n",
    "\n",
    "# -------------------------------\n",
    "# 5. Process Multiple Companies Efficiently\n",
    "# -------------------------------\n",
    "def analyze_multiple_companies(websites, max_workers=4):\n",
    "    results = {}\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        future_to_company = {executor.submit(analyze_company, url, name): name for url, name in websites}\n",
    "        for future in future_to_company:\n",
    "            name = future_to_company[future]\n",
    "            try:\n",
    "                results[name] = future.result()\n",
    "            except Exception as e:\n",
    "                results[name] = {\"summary\": f\"Error: {e}\"}\n",
    "    return results\n",
    "\n",
    "# -------------------------------\n",
    "# Example Usage\n",
    "# -------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    websites = [\n",
    "        (\"https://bathcanalcraft.co.uk\", \"Bath Canal Craft\"),\n",
    "        (\"https://www.kit.edu/\", \"Karlsruhe Institute of Technology\"),\n",
    "        (\"https://mpob.gov.my/\", \"Malaysian Palm Oil Board\"),\n",
    "        (\"https://www.nseindia.com/\", \"NSE India\")\n",
    "    ]\n",
    "\n",
    "    results = analyze_multiple_companies(websites, max_workers=4)\n",
    "    for name, data in results.items():\n",
    "        print(f\"\\n--- {name} ---\\n\")\n",
    "        print(\"Summary:\\n\", data[\"summary\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6bb2a5ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Scraping https://www.bathcanalcraft.co.uk ---\n",
      "\n",
      "{'emails': [], 'phones': [], 'fax': [], 'address': None}\n",
      "\n",
      "--- Scraping https://www.kit.edu/ ---\n",
      "\n",
      "{'emails': [], 'phones': [], 'fax': [], 'address': None}\n",
      "\n",
      "--- Scraping https://www.mpob.gov.my/ ---\n",
      "\n",
      "{'emails': ['general@mpob.gov.my'], 'phones': ['+60 387694400'], 'fax': ['+60 389259446'], 'address': 'Malaysian Palm Oil Board – 6, Persiaran Institusi, Bandar Baru Bangi<br>43000 Kajang Selangor, Malaysia'}\n",
      "\n",
      "--- Scraping https://www.nseindia.com/ ---\n",
      "\n",
      "{'emails': [], 'phones': [], 'fax': [], 'address': None}\n",
      "\n",
      "--- Scraping https://www.research.gla.ac.uk ---\n",
      "\n",
      "Error fetching https://www.research.gla.ac.uk: HTTPSConnectionPool(host='www.research.gla.ac.uk', port=443): Max retries exceeded with url: / (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x00000170F867B250>: Failed to resolve 'www.research.gla.ac.uk' ([Errno 11001] getaddrinfo failed)\"))\n",
      "{}\n",
      "\n",
      "--- Scraping https://www.bebob.de ---\n",
      "\n",
      "Error fetching https://www.bebob.de: HTTPSConnectionPool(host='www.bebob.de', port=443): Max retries exceeded with url: / (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x00000170F804DAD0>, 'Connection to www.bebob.de timed out. (connect timeout=10)'))\n",
      "{}\n",
      "\n",
      "--- Scraping https://www.as.gov.qa ---\n",
      "\n",
      "Error fetching https://www.as.gov.qa: HTTPSConnectionPool(host='www.as.gov.qa', port=443): Max retries exceeded with url: / (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x00000170F86919D0>: Failed to resolve 'www.as.gov.qa' ([Errno 11001] getaddrinfo failed)\"))\n",
      "{}\n",
      "\n",
      "--- Scraping https://www.airjouletech.com ---\n",
      "\n",
      "{'emails': [], 'phones': [], 'fax': [], 'address': None}\n",
      "\n",
      "--- Scraping https://www.moser-konstruktion.de ---\n",
      "\n",
      "Error fetching https://www.moser-konstruktion.de: HTTPSConnectionPool(host='www.moser-konstruktion.de', port=443): Max retries exceeded with url: / (Caused by SSLError(SSLError(1, '[SSL: SSLV3_ALERT_HANDSHAKE_FAILURE] sslv3 alert handshake failure (_ssl.c:1002)')))\n",
      "{}\n",
      "\n",
      "--- Scraping https://www.ac.sce.ac.il ---\n",
      "\n",
      "Error fetching https://www.ac.sce.ac.il: HTTPSConnectionPool(host='www.ac.sce.ac.il', port=443): Max retries exceeded with url: / (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x00000170F8696590>: Failed to resolve 'www.ac.sce.ac.il' ([Errno 11001] getaddrinfo failed)\"))\n",
      "{}\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def normalize_phone(number: str) -> str:\n",
    "    number = number.strip()\n",
    "    number = re.sub(r\"[^\\d+]\", \"\", number)  # keep only digits and '+'\n",
    "\n",
    "    # Malaysia example\n",
    "    if number.startswith(\"603\"):\n",
    "        return \"+60 \" + number[2:]\n",
    "    if number.startswith(\"+60\"):\n",
    "        return number\n",
    "    return number\n",
    "\n",
    "def extract_address(text_lines):\n",
    "    ignore_keywords = [\"skip to content\", \"menu\", \"footer\", \"sitemap\", \"disclaimer\", \"policy\"]\n",
    "    address_keywords = r\"(Persiaran|Bandar|Selangor|Kajang|Malaysia)\"\n",
    "\n",
    "    for i, line in enumerate(text_lines):\n",
    "        line_lower = line.lower()\n",
    "        if any(k in line_lower for k in ignore_keywords):\n",
    "            continue  # skip UI/footer lines\n",
    "\n",
    "        if re.search(r\"\\d+\", line) and re.search(address_keywords, line, re.IGNORECASE):\n",
    "            # include next line if short and not ignored\n",
    "            candidate = line\n",
    "            if i + 1 < len(text_lines):\n",
    "                next_line = text_lines[i+1].strip()\n",
    "                if len(next_line) < 80 and not any(k in next_line.lower() for k in ignore_keywords):\n",
    "                    candidate += \" \" + next_line\n",
    "            return candidate.strip()\n",
    "    return None\n",
    "\n",
    "\n",
    "def scrape_contact_info(url):\n",
    "    try:\n",
    "        resp = requests.get(url, headers={\"User-Agent\": \"Mozilla/5.0\"}, timeout=10)\n",
    "        resp.raise_for_status()\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching {url}: {e}\")\n",
    "        return {}\n",
    "\n",
    "    soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "\n",
    "    # Replace <br> with newline to preserve line separation\n",
    "    for br in soup.find_all(\"br\"):\n",
    "        br.replace_with(\"\\n\")\n",
    "\n",
    "    # Extract visible text lines\n",
    "    text_lines = [line.strip() for line in soup.stripped_strings if line.strip()]\n",
    "\n",
    "    # ----------------- Extract emails -----------------\n",
    "    emails = set()\n",
    "    # from <a href=\"mailto:...\">\n",
    "    for a in soup.find_all(\"a\", href=True):\n",
    "        if a['href'].lower().startswith(\"mailto:\"):\n",
    "            emails.add(a['href'].split(\":\", 1)[1].strip())\n",
    "    # fallback: regex in text\n",
    "    for line in text_lines:\n",
    "        for e in re.findall(r\"[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,}\", line):\n",
    "            emails.add(e)\n",
    "\n",
    "    # ----------------- Extract phones -----------------\n",
    "    phones = set()\n",
    "    for a in soup.find_all(\"a\", href=True):\n",
    "        if a['href'].lower().startswith(\"tel:\"):\n",
    "            phones.add(normalize_phone(a['href'].split(\":\", 1)[1].strip()))\n",
    "    # fallback: regex in text\n",
    "    phone_pattern = r\"(?:Tel|Phone|Telefon)[:.]?\\s*([+0-9\\s\\-]{6,})\"\n",
    "    for line in text_lines:\n",
    "        match = re.search(phone_pattern, line, re.IGNORECASE)\n",
    "        if match:\n",
    "            phones.add(normalize_phone(match.group(1)))\n",
    "\n",
    "    # ----------------- Extract fax -----------------\n",
    "    fax_numbers = set()\n",
    "    fax_pattern = r\"(?:Fax)[:.]?\\s*([+0-9\\s\\-]{6,})\"\n",
    "    for line in text_lines:\n",
    "        match = re.search(fax_pattern, line, re.IGNORECASE)\n",
    "        if match:\n",
    "            fax_numbers.add(normalize_phone(match.group(1)))\n",
    "\n",
    "    # ----------------- Extract address -----------------\n",
    "    address_keywords = r\"(Persiaran|Bandar|Selangor|Kajang|Malaysia)\"\n",
    "    # Replace the previous address parsing with:\n",
    "    address = extract_address(text_lines)\n",
    "\n",
    "    return {\n",
    "        \"emails\": list(emails),\n",
    "        \"phones\": list(phones),\n",
    "        \"fax\": list(fax_numbers),\n",
    "        \"address\": address\n",
    "    }\n",
    "\n",
    "# -------------------------------\n",
    "# Example\n",
    "# -------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    websites = ['https://www.bathcanalcraft.co.uk',\n",
    "        \"https://www.kit.edu/\", \n",
    "        \"https://www.mpob.gov.my/\",\n",
    "        \"https://www.nseindia.com/\", 'https://www.research.gla.ac.uk', \n",
    "        'https://www.bebob.de', 'https://www.as.gov.qa', 'https://www.airjouletech.com',\n",
    "        'https://www.moser-konstruktion.de', 'https://www.ac.sce.ac.il'\n",
    "    ]\n",
    "\n",
    "    for url in websites:\n",
    "        print(f\"\\n--- Scraping {url} ---\\n\")\n",
    "        summary = scrape_contact_info(url)\n",
    "        print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a001854",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: playwright in c:\\users\\mahesh\\spiced\\water_ml_ops\\rubitherm_capstone\\.venv\\lib\\site-packages (1.55.0)\n",
      "Requirement already satisfied: pyee<14,>=13 in c:\\users\\mahesh\\spiced\\water_ml_ops\\rubitherm_capstone\\.venv\\lib\\site-packages (from playwright) (13.0.0)\n",
      "Requirement already satisfied: greenlet<4.0.0,>=3.1.1 in c:\\users\\mahesh\\spiced\\water_ml_ops\\rubitherm_capstone\\.venv\\lib\\site-packages (from playwright) (3.2.4)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\mahesh\\spiced\\water_ml_ops\\rubitherm_capstone\\.venv\\lib\\site-packages (from pyee<14,>=13->playwright) (4.15.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'websites' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 63\u001b[39m\n\u001b[32m     55\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[32m     56\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33memails\u001b[39m\u001b[33m\"\u001b[39m: emails,\n\u001b[32m     57\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mphones\u001b[39m\u001b[33m\"\u001b[39m: phones,\n\u001b[32m     58\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mfax\u001b[39m\u001b[33m\"\u001b[39m: fax_numbers,\n\u001b[32m     59\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33maddress\u001b[39m\u001b[33m\"\u001b[39m: address\n\u001b[32m     60\u001b[39m     }\n\u001b[32m     62\u001b[39m \u001b[38;5;66;03m# Run for all websites in the global 'websites' variable\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m63\u001b[39m results = \u001b[38;5;28;01mawait\u001b[39;00m asyncio.gather(*[scrape_contact_info(url) \u001b[38;5;28;01mfor\u001b[39;00m url \u001b[38;5;129;01min\u001b[39;00m \u001b[43mwebsites\u001b[49m])\n\u001b[32m     64\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m url, data \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(websites, results):\n\u001b[32m     65\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m--- \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m ---\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'websites' is not defined"
     ]
    }
   ],
   "source": [
    "# Ensure playwright is installed\n",
    "%pip install playwright\n",
    "# If running for the first time, uncomment the next line to install browser binaries:\n",
    "# !playwright install\n",
    "\n",
    "import re\n",
    "import asyncio\n",
    "from playwright.async_api import async_playwright\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Use existing variables from the notebook:\n",
    "# ADDRESS_KEYWORDS, PHONE_PATTERN, FAX_PATTERN, EMAIL_PATTERN, IGNORE_KEYWORDS, websites\n",
    "\n",
    "async def scrape_contact_info(url: str):\n",
    "    emails, phones, fax_numbers, address = [], [], [], None\n",
    "    async with async_playwright() as p:\n",
    "        browser = await p.chromium.launch(headless=True)\n",
    "        page = await browser.new_page()\n",
    "        await page.goto(url, timeout=30000)\n",
    "        content = await page.content()  # rendered HTML\n",
    "        await browser.close()\n",
    "\n",
    "    soup = BeautifulSoup(content, \"html.parser\")\n",
    "\n",
    "    # Emails from mailto links\n",
    "    for a in soup.find_all(\"a\", href=True):\n",
    "        if a['href'].startswith(\"mailto:\"):\n",
    "            emails.append(a['href'].split(\":\", 1)[1].strip())\n",
    "\n",
    "    # Emails from text fallback\n",
    "    for line in soup.stripped_strings:\n",
    "        for e in re.findall(EMAIL_PATTERN, line):\n",
    "            emails.append(e.replace(\" \", \"\"))\n",
    "\n",
    "    # Phones\n",
    "    for line in soup.stripped_strings:\n",
    "        match = re.search(PHONE_PATTERN, line, re.IGNORECASE)\n",
    "        if match:\n",
    "            phones.append(match.group(1).strip())\n",
    "\n",
    "    # Fax\n",
    "    for line in soup.stripped_strings:\n",
    "        match = re.search(FAX_PATTERN, line, re.IGNORECASE)\n",
    "        if match:\n",
    "            fax_numbers.append(match.group(1).strip())\n",
    "\n",
    "    # Address (first plausible line)\n",
    "    for line in soup.stripped_strings:\n",
    "        if any(k in line.lower() for k in IGNORE_KEYWORDS):\n",
    "            continue\n",
    "        if re.search(r\"\\d+\", line) and re.search(ADDRESS_KEYWORDS, line, re.IGNORECASE):\n",
    "            address = line.strip()\n",
    "            break\n",
    "\n",
    "    return {\n",
    "        \"emails\": emails,\n",
    "        \"phones\": phones,\n",
    "        \"fax\": fax_numbers,\n",
    "        \"address\": address\n",
    "    }\n",
    "\n",
    "# Run for all websites in the global 'websites' variable\n",
    "websites = ['https://www.bathcanalcraft.co.uk',\n",
    "        \"https://www.kit.edu/\", \n",
    "        \"https://www.mpob.gov.my/\",\n",
    "        \"https://www.nseindia.com/\", 'https://www.research.gla.ac.uk', \n",
    "        'https://www.bebob.de', 'https://www.as.gov.qa', 'https://www.airjouletech.com',\n",
    "        'https://www.moser-konstruktion.de', 'https://www.ac.sce.ac.il'\n",
    "    ]\n",
    "\n",
    "results = await asyncio.gather(*[scrape_contact_info(url) for url in websites])\n",
    "for url, data in zip(websites, results):\n",
    "    print(f\"\\n--- {url} ---\")\n",
    "    print(\"Address:\", data[\"address\"])\n",
    "    print(\"Phones:\", data[\"phones\"])\n",
    "    print(\"Emails:\", data[\"emails\"])\n",
    "    print(\"Fax:\", data[\"fax\"])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
