{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e87c65fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---Company https://bathcanalcraft.co.uk Summary ---\n",
      "\n",
      "Website: https://bathcanalcraft.co.uk\n",
      "\n",
      "Summary: From the initial concept to the maiden voyage, we prioritise your vision every step of the way. Are you looking for a beautifully formed narrowboat, uniquely designed to fit your lifestyle? Join us in redefining the future of narrowboat cruising, where tradition meets innovation in harmony with the environment.\n",
      "\n",
      "Emails: ['bathcanalcraft@icloud.com']\n",
      "Phones: ['+44 7538 784 613']\n",
      "Address: Contact - Bath Canal Craft ‍ ‍ Home Boats All Boats NB ‘Alica-Lee‘ (in planning) NB ‘Fram‘ NB ‘Silly Knot To‘ NB ‘Slapdash Lilly‘ NB ‘The Doran‘ (sailaway) NB ‘Elaine‘ NB ‘Velore‘ Contact If you’d like to discuss a personalised narrowboat build then please feel free call, email, or come visit… +44 7538 784 613 bathcanalcraft@icloud.com Bath Canal Craft Ltd. Deverill Storage Longbridge Deverill Warminster Wiltshire BA12 7FB Bath Canal Craft Ltd. | All Rights Reserved © 2025\n",
      "\n",
      "---Company https://www.kit.edu/ Summary ---\n",
      "\n",
      "Website: https://www.kit.edu/\n",
      "\n",
      "Summary: Die Mission des KIT ist Teil der Dachstrategie KIT 2025 . Die Ausstellung „Wer entscheidet denn sowas? Zur Eröffnung lädt das ITAS am Donnerstag, 4.\n",
      "\n",
      "Emails: ['internetredaktion@sts.kit.edu', 'info@kit.edu']\n",
      "Phones: ['266749428', '49 721 608', '+49 721 608-44290']\n",
      "Address: KIT - Das KIT - Impressum Karlsruher Institut für Technologie Navigation überspringen Home Leichte Sprache Gebärdensprache Impressum Datenschutz Barrierefreiheit Sitemap Intranet KIT en suchen suchen Das KIT Start Das KIT Das KIT Profil Start Das KIT Profil Profil Mission Exzellenz Start Das KIT Profil Exzellenz Exzellenz Exzellenzuniversität Exzellenzcluster Chancengleichheit und Diversität Nachhaltigkeit Forschungsuniversität in der Helmholtz-Gemeinschaft Zahlen, Daten, Fakten Start Das KIT Profil Zahlen, Daten, Fakten Zahlen, Daten, Fakten Rankings Organisation Start Das KIT Organisation Organisation Leitung Organe und Gremien Start Das KIT Organisation Organe und Gremien Organe und Gremien Aufsichtsrat Präsidium Bereichsleitung KIT-Senat Konvent Personalvertretungen Verfasste Studierendenschaft Bereiche Start Das KIT Organisation Bereiche Bereiche Institute KIT-Fakultäten HGF-Programme KIT-Zentren Verwaltung und Infrastruktur Projektträger Karlsruhe Menschen Start Das KIT Menschen Menschen Persönlichkeiten Geschichte Spitzenforschende Start Das KIT Menschen Spitzenforschende Spitzenforschende Alexander von Humboldt-Stiftung European Research Council Deutsche Forschungsgemeinschaft Engagement Start Das KIT Engagement Engagement KIT GIVING KIT-Stiftung Sponsoring KIT-Alumni KFG Medien Start Das KIT Medien Medien Presseinformationen Start Das KIT Medien Presseinformationen Presseinformationen PI 2025 Archiv Presseinformationen News Start Das KIT Medien News News News 2025 Themenhighlights Social Media Jahresberichte Publikationen Podcasts Rund um das KIT Start Das KIT Rund um das KIT Rund um das KIT 200 Jahre KIT KIT für Alle Kultur und Sport Campusführungen Veranstaltungen Start Das KIT Rund um das KIT Veranstaltungen Veranstaltungen Veranstaltungskalender Standorte und Anfahrt Start Das KIT Standorte und Anfahrt Standorte und Anfahrt Interaktiver Campusplan Lagepläne Themen Start Themen Themen Energie Mobilität Information Klima und Umwelt Materialien Mensch und Technik Elementar- und Astroteilchenphysik Mathematik in der Anwendung Studium Start Studium Studium Vor dem Studium Start Studium Vor dem Studium Vor dem Studium Studiengänge Bewerbung Rat und Hilfe Leben und Lernen in Karlsruhe Internationale Studierende Im Studium Start Studium Im Studium Im Studium Studentische Einrichtungen Intranet Nach dem Studium Start Studium Nach dem Studium Nach dem Studium Formalitäten Master und Promotion Berufseinstieg In Verbindung bleiben Weiterbildung KIT-Fakultäten Rat und Hilfe Leben und Lernen in Karlsruhe Start Studium Leben und Lernen in Karlsruhe Leben und Lernen in Karlsruhe Leben in Karlsruhe Lernen am KIT Unterwegs in Karlsruhe Forschung Start Forschung Forschung Themen Start Forschung Themen Themen Energie Mobilität Information Klima und Umwelt Materialien Mensch und Technik Elementar- und Astroteilchenphysik Mathematik in der Anwendung Exzellenz Start Forschung Exzellenz Exzellenz Exzellenzuniversität Exzellenzcluster Helmholtz-Programme Forschungsverbünde Start Forschung Forschungsverbünde Forschungsverbünde Sonderforschungsbereiche DFG-Forschungsgruppen Wissenschaftlicher Nachwuchs Start Forschung Wissenschaftlicher Nachwuchs Wissenschaftlicher Nachwuchs Qualifikation zur Professur Start Forschung Wissenschaftlicher Nachwuchs Qualifikation zur Professur Qualifikation zur Professur Young Investigator Group Preparation Program KIT-Nachwuchsgruppe KIT Associate Fellow Tenure-Track-Professur Postdoc-Phase Promotion Start Forschung Wissenschaftlicher Nachwuchs Promotion Promotion Promovieren am KIT Promotionsprogramme Forschungsinfrastrukturen Gute wissenschaftliche Praxis Innovation Start Innovation Innovation Technologietransfer Kooperation Innovation in Forschung und Lehre Gründen Karriere Start Karriere Karriere Wissenschaft Start Karriere Wissenschaft Wissenschaft Professuren Wissenschaftliche Mitarbeitende Wissenschaftlicher Nachwuchs Start Karriere Wissenschaft Wissenschaftlicher Nachwuchs Wissenschaftlicher Nachwuchs Promotion Postdoktorat Qualifikation zur Professur Verwaltung und Infrastruktur Start Karriere Verwaltung und Infrastruktur Verwaltung und Infrastruktur IT Technische Infrastruktur Start Karriere Verwaltung und Infrastruktur Technische Infrastruktur Technische Infrastruktur Trainee technische Infrastruktur Schüler und Studierende Start Karriere Schüler und Studierende Schüler und Studierende Ausbildung und Duales Studium Angebote für Schülerinnen und Schüler Start Karriere Schüler und Studierende Angebote für Schülerinnen und Schüler Angebote für Schülerinnen und Schüler Schülerlabore Studienbotschafterinnen Start Karriere Schüler und Studierende Angebote für Schülerinnen und Schüler Studienbotschafterinnen Studienbotschafterinnen Architektur Bauingenieur-, Geo- und Umweltwissenschaften Chemieingenieurwesen und Verfahrenstechnik Elektrotechnik und Informationstechnik Geistes- und Sozialwissenschaften Mathematik Physik KIT als Arbeitgeber Bewerbungsprozess Stellenangebote Startseite Home Leichte Sprache Gebärdensprache Impressum Datenschutz Barrierefreiheit Sitemap Intranet suchen suchen Startseite Das KIT Karlsruher Institut für Technologie Das KIT Themen Studium Forschung Innovation Karriere Das KIT Profil Organisation Menschen Engagement Medien Rund um das KIT Standorte und Anfahrt Impressum Anbieter Copyright Haftung Verweise auf externe Web-Seiten Datenschutz und IT-Sicherheit Anbieter Diensteanbieter im Sinne von §5 Digitale Dienste Gesetz (DDG) und §18 Abs. 2 Medienstaatsvertrag (MStV): Rechtlicher Sitz: Karlsruher Institut für Technologie Kaiserstraße 12 76131 Karlsruhe Deutschland Tel.: +49 721 608-0 Fax: +49 721 608-44290 E-Mail: info@kit.edu Rechtsform: Körperschaft des öffentlichen Rechts Vertretungsberechtigt: Prof. Dr. Jan S. Hesthaven (Präsident des KIT) Verantwortlich für den Inhalt: Margarete Lehné Leiterin Gesamtkommunikation Stab und Strategie Anfragen zum Inhalt E-Mail: internetredaktion@sts.kit.edu Umsatzsteueridentifikationsnummer: DE266749428 Copyright Für die Internet-Seiten des Karlsruher Instituts für Technologie liegen Copyright und alle weiteren Rechte beim Karlsruher Institut für Technologie, Kaiserstraße 12, 76131 Karlsruhe, Deutschland. Weiterverbreitung, auch in Auszügen, für pädagogische, wissenschaftliche oder private Zwecke ist unter Angabe der Quelle gestattet (sofern nicht anders an der entsprechenden Stelle ausdrücklich angegeben). Eine Verwendung im gewerblichen Bereich bedarf der Genehmigung durch das Karlsruher Institut für Technologie. Ansprechpartner ist die Dienstleistungseinheit Stab und Strategie . Haftung Diese Internetseiten dienen lediglich der Information. Ihr Inhalt wurde mit gebührender Sorgfalt zusammengestellt. Das Karlsruher Institut für Technologie übernimmt aber keine Garantie, weder ausdrücklich noch implizit, für die Art oder Richtigkeit des dargebotenen Materials und übernimmt keine Haftung (einschließlich Haftung für indirekten Verlust oder Gewinn- oder Umsatzverluste) bezüglich des Materials bzw. der Nutzung dieses Materials. Sollten Inhalte von Web-Seiten des Karlsruher Instituts für Technologie gegen geltende Rechtsvorschriften verstoßen, dann bitten wir um umgehende Benachrichtigung. Wir werden die Seite oder den betreffenden Inhalt dann schnellstmöglich entfernen. Verweise auf externe Web-Seiten Die Web-Seiten des Karlsruher Instituts für Technologie enthalten Verweise (Links) zu Informationsangeboten auf Servern, die nicht der Kontrolle und Verantwortlichkeit des Karlsruher Instituts für Technologie unterliegen. Das Karlsruher Institut für Technologie übernimmt keine Verantwortung und keine Garantie für diese Informationen und billigt oder unterstützt diese auch nicht inhaltlich. Datenschutz und IT-Sicherheit Hinweise zum Datenschutz finden Sie in unserer Datenschutzerklärung. Sollten Sie weitere Fragen zu unseren Datenschutz-Standards haben, wenden Sie sich bitte an unseren Datenschutzbeauftragten . Möchten Sie sich über die Konzepte und technische Realisierung unserer IT-Sicherheit informieren, wenden Sie sich bitte an unseren Informationssicherheitsbeauftragten . Feedback Ihr Feedback zu dieser Seite ist uns wichtig * Name * E-Mail Adresse * Ihre Nachricht * Captcha Senden * Pflichtfeld letzte Änderung: 21.08.2025 KIT – Die Forschungsuniversität in der Helmholtz-Gemeinschaft Home Leichte Sprache Gebärdensprache Impressum Datenschutz Barrierefreiheit Sitemap Intranet KIT\n",
      "\n",
      "---Company https://mpob.gov.my/ Summary ---\n",
      "\n",
      "Website: https://mpob.gov.my/\n",
      "\n",
      "Summary: Our mission To enhance the well-being of the Malaysian oil palm industry through excellent research & development and services. Conduct research and development related to the oil palm industry. Function Implement policies and development programmes to ensure viability of the oil palm industry of Malaysia.\n",
      "\n",
      "Emails: ['general@mpob.gov.my']\n",
      "Phones: ['603-89259446', '603-87694400']\n",
      "Address: Malaysian Palm Oil Board – 6, Persiaran Institusi, Bandar Baru Bangi<br>43000 Kajang Selangor, Malaysia Skip to content Malaysian Palm Oil Board 6, Persiaran Institusi, Bandar Baru Bangi 43000 Kajang Selangor, Malaysia CERTIFIED TO ISO 9001:2015 CERT. NO.: QMS 02602 Contact Us Sitemap Menu Corporate Info About Us MPOB Logo Vision & Mission Organisation Chart Clientsâ Charter Top Management Board Members Division Research & Development (R&D) Advanced Biotechnology and Breeding CentreÂ Biology Sustainability Research Division Engineering & Processing Research Division Advanced Oleochemical Technology Division Product Development & Advisory Services Division Services Economics & Industry Development Division Licensing & Enforcement Division Smallholder Extension & Certification Division Information Technology & Corporate Services Division Management, Finance & Development Division eServices Industry Application MyLesen e-LesenPK SIMS e-Kilang e-Peniaga e-Submission e-Registration Online Application PALMOILIS Genom Sawit GanoID Mobile application Info Sawit Sawit Secure Public Access Statistic of Online Services (2025) Dataset Internship MSPO Unit Integriti Perjanjian Perkongsian Ekonomi Malaysia-EFTA (MEEPA) Conferences & Courses Conference / Seminars Nutrition Satellite Symposium in conjunction with PIPOC 2025 ISOPB - Advancing The Next Generation of Oil Palm Planting Materials & Annual General Meeting MPOB International Palm Oil Congress and Exhibition (PIPOC) 2025 International Conference on Oil Palm Plant Protection (ICOPP) 2025 Palm Oil Economic Review and Outlook Seminar R&O 2026 Courses List of Courses Semakan Keputusan Peperiksaan Training Calendar Media News Press Release Speeches Photo Gallery Corporate Video Publication Online Journals Journal of Oil Palm Research Journal (JOPR) Oil Palm Industry Economic Journal (OPIEJ) Bulletin Palm Oil Engineering Bulletin (POEB) Warta Sawit eBook MPOB eBook Red Palm Oil eBook Printed Laporan Tahunan MPOB Books MPOB Publication Catalogue 2025 Advances In Oil Palm Research - Second Edition Contact us MPOB Offices Staff Directory Local Office PORTSIM 25th Anniversary Selamat Menyambut Hari Kebangsaan ke-68 31 Ogos 2025 2025_Perpindahan_1 Slider Malaysian Standard 1.8.2025 Slide 2 ISOPB Banner 2025_Nutrition Satellite 3 2025_ICOPP Banner 1920 x 691 (3) Website R&O 2026 2026_PKPKS_2 MPOB Technology Awards and Recognition chart-line-up Daily CPO Prices MYR 4,337.50 3 September 2025 Smallholder Scheme Tenders & Quotations MARCOP MyGOV KSN KPK Email: general@mpob.gov.my Tel: 603-87694400 Fax: 603-89259446 Facebook X-twitter Youtube Instagram Tiktok Disclaimer | Security Policy | Privacy Policy © 2025 Malaysian Palm Oil Board Type your Message X Welcome to MPOB × Ã Table of Contents Table of Contents Search for: PIPOC 2025 Red Palm Oil eBook Printed Publication SPOTME Number of Online Services Privacy Policy Dasar Privasi\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from urllib.parse import urljoin\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "def scrape_text(url):\n",
    "    \"\"\"Fetch and clean visible text from a webpage.\"\"\"\n",
    "    try:\n",
    "        resp = requests.get(url, headers={\"User-Agent\": \"Mozilla/5.0\"}, timeout=10)\n",
    "        resp.raise_for_status()\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching {url}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "    soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "    for tag in soup([\"script\", \"style\", \"noscript\"]):\n",
    "        tag.decompose()\n",
    "\n",
    "    text = soup.get_text(separator=\" \", strip=True)\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    return text\n",
    "\n",
    "def find_relevant_pages(base_url, keywords):\n",
    "    \"\"\"Find pages by keyword relevance.\"\"\"\n",
    "    try:\n",
    "        resp = requests.get(base_url, headers={\"User-Agent\": \"Mozilla/5.0\"}, timeout=10)\n",
    "        resp.raise_for_status()\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching {base_url}: {e}\")\n",
    "        return [base_url]\n",
    "\n",
    "    soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "    links = [a.get(\"href\") for a in soup.find_all(\"a\", href=True)]\n",
    "\n",
    "    relevant_pages = []\n",
    "    for link in links:\n",
    "        if any(k in link.lower() for k in keywords):\n",
    "            full_link = urljoin(base_url, link)\n",
    "            if full_link not in relevant_pages:\n",
    "                relevant_pages.append(full_link)\n",
    "\n",
    "    return relevant_pages\n",
    "\n",
    "def summarize_text(text, max_sentences=3):\n",
    "    \"\"\"Generate a short, crisp company summary.\"\"\"\n",
    "    sentences = re.split(r'(?<=[.!?]) +', text)\n",
    "    keywords = [\"about\", \"vision\", \"mission\", \"objective\", \"focus\", \"establish\",\n",
    "                \"function\", \"research\", \"development\", \"sustain\", \"industry\"]\n",
    "\n",
    "    scored = []\n",
    "    for s in sentences:\n",
    "        score = sum(k in s.lower() for k in keywords)\n",
    "        if 40 < len(s) < 250:  # avoid junk sentences\n",
    "            scored.append((score, s.strip()))\n",
    "\n",
    "    scored.sort(key=lambda x: (-x[0], len(x[1])))\n",
    "    top_sentences = [s for _, s in scored[:max_sentences]]\n",
    "    return \" \".join(top_sentences)\n",
    "\n",
    "def extract_contacts(text):\n",
    "    \"\"\"Extract emails, phone numbers, and possible addresses.\"\"\"\n",
    "    emails = list(set(re.findall(r\"[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,}\", text)))\n",
    "    phones = list(set(re.findall(r\"(?:\\+?\\d{1,3}[-.\\s]?)?\\(?\\d{2,4}\\)?[-.\\s]?\\d{3,5}[-.\\s]?\\d{3,5}\", text)))\n",
    "\n",
    "    # crude heuristic for address\n",
    "    address_candidates = re.findall(r\"([A-Z][^,]+(?:Street|Strasse|Road|Rd|Avenue|Ave|Lane|Ln|Boulevard|Blvd|Way|Place|Pl|Square|Sq|Drive|Dr|Building|Bldg|No\\.|Kaiserstraße).+)\", text, re.IGNORECASE)\n",
    "    address = address_candidates[0] if address_candidates else None\n",
    "\n",
    "    return {\n",
    "        \"emails\": emails,\n",
    "        \"phones\": phones,\n",
    "        \"address\": address\n",
    "    }\n",
    "\n",
    "def company_profile(base_url):\n",
    "    \"\"\"Scrape summary + contacts separately for better results.\"\"\"\n",
    "    # Pages for summary\n",
    "    summary_pages = [base_url] + find_relevant_pages(base_url, [\"about\", \"vision\", \"mission\", \"who-we-are\", \"company\", \"corporate\"])\n",
    "    \n",
    "    # Pages for contact info\n",
    "    contact_pages = find_relevant_pages(base_url, [\"contact\", \"impressum\", \"imprint\"])\n",
    "    if not contact_pages:\n",
    "        contact_pages = [base_url]  # fallback\n",
    "\n",
    "    # Build summary\n",
    "    summary_text = \"\"\n",
    "    for page in summary_pages[:5]:\n",
    "        summary_text += \" \" + scrape_text(page)\n",
    "    summary = summarize_text(summary_text)\n",
    "\n",
    "    # Extract contact info\n",
    "    contact_text = \"\"\n",
    "    for page in contact_pages[:3]:\n",
    "        contact_text += \" \" + scrape_text(page)\n",
    "    contacts = extract_contacts(contact_text)\n",
    "\n",
    "    return {\n",
    "        \"url\": base_url,\n",
    "        \"summary\": summary,\n",
    "        \"contacts\": contacts\n",
    "    }\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    urls = [\"https://bathcanalcraft.co.uk\",\"https://www.kit.edu/\",\"https://mpob.gov.my/\"] \n",
    "    for url in urls:\n",
    "        print(f\"\\n---Company {url} Summary ---\\n\")\n",
    "        profile = company_profile(url)\n",
    "        print(\"Website:\", profile[\"url\"])\n",
    "        print(\"\\nSummary:\", profile[\"summary\"])\n",
    "        print(\"\\nEmails:\", profile[\"contacts\"][\"emails\"])\n",
    "        print(\"Phones:\", profile[\"contacts\"][\"phones\"])\n",
    "        print(\"Address:\", profile[\"contacts\"][\"address\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c87fdfc8",
   "metadata": {},
   "source": [
    "***Smart summary about company using facebook HuggingFace model.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da3f047",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Scraping https://www.bathcanalcraft.co.uk ---\n",
      "\n",
      "Summary:\n",
      " **https://www.bathcanalcraft.co.uk**: Bath Canal Craft offers a truly bespoke service, working closely with you to ensure you have control over the design decisions that matter most. Join us in redefining the future of narrowboat cruising, where tradition meets innovation in harmony with the environment.\n",
      "\n",
      "--- Scraping https://www.kit.edu/ ---\n",
      "\n",
      "Summary:\n",
      " **https://www.kit.edu/**: KIT - Karlsruher Institut für Technologie Navigation überspringen Home Leichte Sprache Gebärdensprache Impressum Datenschutz Barrierefreiheit Sitemap Intranet KIT en suchen suchen.\n",
      "\n",
      "--- Scraping https://www.mpob.gov.my/ ---\n",
      "\n",
      "Summary:\n",
      " **https://www.mpob.gov.my/**: Malaysian Palm Oil Board – 6, Persiaran Institusi, Bandar Baru Bangi 43000 Kajang Selangor, Malaysia CERTIFIED TO ISO 9001:2015 CERT. NO.: QMS 02602 Contact Us Sitemap Menu Corporate Info About Us MPOB Logo Vision & Mission Organisation Chart Clientsâ Charter Top Management Board Members Division Research & Development (R&D) Advanced Biotechnology and Breeding CentreÂ Biology Sustainability Research Division Engineering & Processing Research Division Advanced Oleochemical Technology Division Product Development & Advisory\n",
      "\n",
      "--- Scraping https://www.nseindia.com/ ---\n",
      "\n",
      "Summary:\n",
      " **https://www.nseindia.com/**: NSE - National Stock Exchange of India Ltd: Live Share/Stock Market News &amp; Updates, Quotes- Nseindia.com Option Chain Market Turnover Listings IPO Circulars Daily Report Holidays Corporates Press Releases Contact Us English 24,715.05 135.45 ( 0.55 %)\n",
      "\n",
      "--- Scraping https://www.research.gla.ac.uk ---\n",
      "\n",
      "Normal request failed: HTTPSConnectionPool(host='www.research.gla.ac.uk', port=443): Max retries exceeded with url: / (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x000001EADD1E0D50>: Failed to resolve 'www.research.gla.ac.uk' ([Errno 11001] getaddrinfo failed)\"))\n",
      "Summary:\n",
      " None\n",
      "\n",
      "--- Scraping https://www.bebob.de ---\n",
      "\n",
      "Normal request failed: HTTPSConnectionPool(host='www.bebob.de', port=443): Max retries exceeded with url: / (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x000001EADDDE3FD0>, 'Connection to www.bebob.de timed out. (connect timeout=10)'))\n",
      "Summary:\n",
      " None\n",
      "\n",
      "--- Scraping https://www.as.gov.qa ---\n",
      "\n",
      "Normal request failed: HTTPSConnectionPool(host='www.as.gov.qa', port=443): Max retries exceeded with url: / (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x000001EADD11B610>: Failed to resolve 'www.as.gov.qa' ([Errno 11001] getaddrinfo failed)\"))\n",
      "Summary:\n",
      " None\n",
      "\n",
      "--- Scraping https://www.airjouletech.com ---\n",
      "\n",
      "Summary:\n",
      " **https://www.airjouletech.com**: AirJoule® - Transformational Cooling & Water Technology. Main Navigation Technology Air In. Water Out. A1000 Solutions Water from Air Water Recovery Cooling Systems Moisture Control Applications Data Centers Company About AirJouLE Our Partners Careers News Investors Contact Home THE POWER OF WATER FROM AIR home-data-center 5 million gallons of water per day goes to COOLING A DATA CENTER.\n",
      "\n",
      "--- Scraping https://www.moser-konstruktion.de ---\n",
      "\n",
      "Normal request failed: HTTPSConnectionPool(host='www.moser-konstruktion.de', port=443): Max retries exceeded with url: / (Caused by SSLError(SSLError(1, '[SSL: SSLV3_ALERT_HANDSHAKE_FAILURE] sslv3 alert handshake failure (_ssl.c:1002)')))\n",
      "Summary:\n",
      " None\n",
      "\n",
      "--- Scraping https://www.ac.sce.ac.il ---\n",
      "\n",
      "Normal request failed: HTTPSConnectionPool(host='www.ac.sce.ac.il', port=443): Max retries exceeded with url: / (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x000001EADD0A9310>: Failed to resolve 'www.ac.sce.ac.il' ([Errno 11001] getaddrinfo failed)\"))\n",
      "Summary:\n",
      " None\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from transformers import pipeline\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.filterwarnings(\"ignore\", message=\".*max_new_tokens.*\")\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "# Optional JS rendering\n",
    "try:\n",
    "    from requests_html import HTMLSession\n",
    "    JS_RENDER_AVAILABLE = True\n",
    "except ImportError:\n",
    "    JS_RENDER_AVAILABLE = False\n",
    "\n",
    "# Load local summarizer\n",
    "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "# summarizer = pipeline(\"summarization\", model=\"t5-small\")\n",
    "# -------------------------------\n",
    "# 1. Scrape Website Text\n",
    "# -------------------------------\n",
    "def scrape_website(url):\n",
    "    text = \"\"\n",
    "    try:\n",
    "        r = requests.get(url, headers={\"User-Agent\": \"Mozilla/5.0\"}, timeout=10)\n",
    "        soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "        for script in soup([\"script\", \"style\", \"noscript\"]):\n",
    "            script.extract()\n",
    "        text = \" \".join(soup.stripped_strings)\n",
    "    except Exception as e:\n",
    "        print(f\"Normal request failed: {e}\")\n",
    "\n",
    "    # Fallback to JS-rendered\n",
    "    if len(text.split()) < 50 and JS_RENDER_AVAILABLE:\n",
    "        try:\n",
    "            session = HTMLSession()\n",
    "            r = session.get(url)\n",
    "            r.html.render(timeout=20)\n",
    "            text = r.html.text\n",
    "        except Exception as e:\n",
    "            print(f\"JS render failed: {e}\")\n",
    "\n",
    "    return text\n",
    "\n",
    "# -------------------------------\n",
    "# 3. Extract Objectives and Services\n",
    "# -------------------------------\n",
    "def extract_objective_service_text(text):\n",
    "    paragraphs = [p.strip() for p in text.split(\"\\n\") if len(p.strip()) > 40]\n",
    "\n",
    "    # Keywords\n",
    "    objective_keywords = [\"mission\", \"objective\", \"goal\", \"vision\", \"purpose\", \"focus\", \"aim\"]\n",
    "    service_keywords = [\"service\", \"product\", \"offer\", \"specialize\", \"provide\", \"solutions\", \"design\", \"build\", \"manufacture\"]\n",
    "\n",
    "    # Extract paragraphs\n",
    "    objective_paragraphs = [p for p in paragraphs if any(k in p.lower() for k in objective_keywords)]\n",
    "    service_paragraphs = [p for p in paragraphs if any(k in p.lower() for k in service_keywords)]\n",
    "\n",
    "    combined = objective_paragraphs + service_paragraphs\n",
    "    if combined:\n",
    "        return \" \".join(combined)\n",
    "    else:\n",
    "        # fallback to first few paragraphs\n",
    "        return \" \".join(paragraphs[:10])\n",
    "\n",
    "# -------------------------------\n",
    "# 4. Generate Crisp Summary\n",
    "# -------------------------------\n",
    "def generate_summary(text, company_name=\"The company\"):\n",
    "    text = text[:2000]  # HuggingFace token limit\n",
    "    input_length = len(text.split())\n",
    "    max_length = min(120, input_length)  # Ensure max_length <= input_length\n",
    "    min_length = min(40, max_length - 1) if max_length > 40 else 10\n",
    "    result = summarizer(text, max_length=max_length, min_length=min_length, do_sample=False)\n",
    "    return f\"**{company_name}**: {result[0]['summary_text']}\"\n",
    "\n",
    "# -------------------------------\n",
    "# 5. Full Company Analyzer\n",
    "# -------------------------------\n",
    "def     analyze_company(url, company_name=\"The company\"):\n",
    "    text = scrape_website(url)\n",
    "    objective_service_text = extract_objective_service_text(text)\n",
    "    if objective_service_text:\n",
    "        summary = generate_summary(objective_service_text, company_name)\n",
    "    else:\n",
    "        summary = \"None\"\n",
    "    \n",
    "    return summary\n",
    "    \n",
    "\n",
    "# -------------------------------\n",
    "# Example Usage\n",
    "# -------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    websites = ['https://www.asml.com', 'https://www.liwest.at', 'https://www.novem.com', 'https://www.steinhaus.net', 'https://www.ost.ch', 'https://www.audi.de', 'https://www.landpack.de', 'https://www.schaumaplast.de', 'https://www.zukunfts.haus', 'https://www.googlemail.com', 'https://www.federation.edu.au', 'https://www.ciiae.org', 'https://www.hb-ingenieure.de', 'https://www.windowslive.com', 'https://www.kyocera.jp', 'https://www.elite-tec.com.cn', 'https://www.fineeng.eu', 'https://www.influtherm.com', 'https://www.lengheim-entwicklung.at', 'https://www.synthesis-spa.com', 'https://www.jimdo.de', 'https://www.sonoco.com', 'https://www.izeau.fr', 'https://www.mas-sp.pl', 'https://www.kreon.com',\n",
    "            'https://www.braebo.lu', 'https://www.tamu.edu', 'https://www.lms-germany.de', 'https://www.lifebiotek.com', 'https://www.tuke.sk', 'https://www.adelaide.edu.au', 'https://www.greyb.com', 'https://www.tuke.sk', 'https://www.heatventors.com', 'https://www.unimore.it', 'https://www.ua.pt', 'https://www.puffinpackaging.co.uk', 'https://www.voltaspace.co', 'https://www.altileo.com', 'https://www.eaf.org.br', 'https://www.cilas.com', 'https://www.lut.fi', 'https://www.utwente.nl', 'https://www.intudiagnostics.com', 'https://www.ucl.ac.uk', 'https://www.sanbs.org.za', 'https://www.rit.edu', 'https://www.pole-cristal.fr', 'https://www.uni-freiburg.de', 'https://www.fusemat.com']\n",
    "\n",
    "\n",
    "    for url in websites:\n",
    "        print(f\"\\n--- Scraping {url} ---\\n\")\n",
    "        summary = analyze_company(url)\n",
    "        print(summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0e1201e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Mahesh\\Spiced\\water_ml_ops\\Rubitherm_Capstone\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Device set to use cpu\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normal request failed for http://schaumaplast.de/: HTTPConnectionPool(host='schaumaplast.de', port=80): Max retries exceeded with url: / (Caused by NameResolutionError(\"<urllib3.connection.HTTPConnection object at 0x000001E65248E610>: Failed to resolve 'schaumaplast.de' ([Errno 11001] getaddrinfo failed)\"))\n",
      "\n",
      "--- Bath Canal Craft ---\n",
      "\n",
      "Summary:\n",
      " None\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from transformers import pipeline\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "# Optional JS rendering\n",
    "try:\n",
    "    from requests_html import HTMLSession\n",
    "    JS_RENDER_AVAILABLE = True\n",
    "except ImportError:\n",
    "    JS_RENDER_AVAILABLE = False\n",
    "\n",
    "# -------------------------------\n",
    "# Load summarizer once\n",
    "# -------------------------------\n",
    "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "# summarizer = pipeline(\"summarization\", model=\"t5-small\")\n",
    "# -------------------------------\n",
    "# 1. Scrape Website Text\n",
    "# -------------------------------\n",
    "def scrape_website(url):\n",
    "    \"\"\"Fetch visible text from webpage with headers to avoid 403 errors.\"\"\"\n",
    "    text = \"\"\n",
    "    try:\n",
    "        r = requests.get(url, headers={\"User-Agent\": \"Mozilla/5.0\"}, timeout=10)\n",
    "        r.raise_for_status()\n",
    "        soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "        for tag in soup([\"script\", \"style\", \"noscript\"]):\n",
    "            tag.decompose()\n",
    "        text = \" \".join(soup.stripped_strings)\n",
    "    except Exception as e:\n",
    "        print(f\"Normal request failed for {url}: {e}\")\n",
    "\n",
    "    # Fallback to JS-rendered\n",
    "    if len(text.split()) < 50 and JS_RENDER_AVAILABLE:\n",
    "        try:\n",
    "            session = HTMLSession()\n",
    "            r = session.get(url)\n",
    "            r.html.render(timeout=20)\n",
    "            text = r.html.text\n",
    "        except Exception as e:\n",
    "            print(f\"JS render failed for {url}: {e}\")\n",
    "\n",
    "    return text\n",
    "\n",
    "# -------------------------------\n",
    "# 2. Extract Objectives & Services\n",
    "# -------------------------------\n",
    "def extract_objective_service_text(text):\n",
    "    paragraphs = [p.strip() for p in text.split(\"\\n\") if len(p.strip()) > 40]\n",
    "\n",
    "    objective_keywords = [\"mission\", \"objective\", \"goal\", \"vision\", \"purpose\", \"focus\", \"aim\"]\n",
    "    service_keywords = [\"service\", \"product\", \"offer\", \"specialize\", \"provide\", \"solutions\", \"design\", \"build\", \"manufacture\"]\n",
    "\n",
    "    objective_paragraphs = [p for p in paragraphs if any(k in p.lower() for k in objective_keywords)]\n",
    "    service_paragraphs = [p for p in paragraphs if any(k in p.lower() for k in service_keywords)]\n",
    "\n",
    "    combined = objective_paragraphs + service_paragraphs\n",
    "    if combined:\n",
    "        return \" \".join(combined)\n",
    "    return \" \".join(paragraphs[:10]) if paragraphs else \"\"\n",
    "\n",
    "# -------------------------------\n",
    "# 3. Generate Crisp Summary\n",
    "# -------------------------------\n",
    "def generate_summary(text, company_name=\"The company\"):\n",
    "    text = text[:2000]  # HuggingFace token limit\n",
    "    input_length = len(text.split())\n",
    "    max_length = min(120, input_length)\n",
    "    min_length = min(40, max_length - 1) if max_length > 40 else 10\n",
    "    result = summarizer(text, max_length=max_length, min_length=min_length, do_sample=False)\n",
    "    return f\"**{company_name}**: {result[0]['summary_text']}\"\n",
    "\n",
    "# -------------------------------\n",
    "# 4. Full Company Analyzer\n",
    "# -------------------------------\n",
    "def analyze_company(url, company_name=\"The company\"):\n",
    "    text = scrape_website(url)\n",
    "    objective_service_text = extract_objective_service_text(text)\n",
    "    summary = generate_summary(objective_service_text, company_name) if objective_service_text else \"None\"\n",
    "    return {\n",
    "        \"summary\": summary\n",
    "    }\n",
    "\n",
    "# -------------------------------\n",
    "# 5. Process Multiple Companies Efficiently\n",
    "# -------------------------------\n",
    "def analyze_multiple_companies(websites, max_workers=4):\n",
    "    results = {}\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        future_to_company = {executor.submit(analyze_company, url, name): name for url, name in websites}\n",
    "        for future in future_to_company:\n",
    "            name = future_to_company[future]\n",
    "            try:\n",
    "                results[name] = future.result()\n",
    "            except Exception as e:\n",
    "                results[name] = {\"summary\": f\"Error: {e}\"}\n",
    "    return results\n",
    "\n",
    "# -------------------------------\n",
    "# Example Usage\n",
    "# -------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    websites = [\n",
    "        (\"http://schaumaplast.de/\", \"Bath Canal Craft\"),\n",
    "        # (\"https://www.kit.edu/\", \"Karlsruhe Institute of Technology\"),\n",
    "        # (\"https://mpob.gov.my/\", \"Malaysian Palm Oil Board\"),\n",
    "        # (\"https://www.nseindia.com/\", \"NSE India\")\n",
    "    ]\n",
    "\n",
    "    results = analyze_multiple_companies(websites, max_workers=4)\n",
    "    for name, data in results.items():\n",
    "        print(f\"\\n--- {name} ---\\n\")\n",
    "        print(\"Summary:\\n\", data[\"summary\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6bb2a5ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Scraping https://www.bathcanalcraft.co.uk ---\n",
      "\n",
      "{'emails': [], 'phones': [], 'fax': [], 'address': None}\n",
      "\n",
      "--- Scraping https://www.kit.edu/ ---\n",
      "\n",
      "{'emails': [], 'phones': [], 'fax': [], 'address': None}\n",
      "\n",
      "--- Scraping https://www.mpob.gov.my/ ---\n",
      "\n",
      "{'emails': ['general@mpob.gov.my'], 'phones': ['+60 387694400'], 'fax': ['+60 389259446'], 'address': 'Malaysian Palm Oil Board – 6, Persiaran Institusi, Bandar Baru Bangi<br>43000 Kajang Selangor, Malaysia'}\n",
      "\n",
      "--- Scraping https://www.nseindia.com/ ---\n",
      "\n",
      "{'emails': [], 'phones': [], 'fax': [], 'address': None}\n",
      "\n",
      "--- Scraping https://www.research.gla.ac.uk ---\n",
      "\n",
      "Error fetching https://www.research.gla.ac.uk: HTTPSConnectionPool(host='www.research.gla.ac.uk', port=443): Max retries exceeded with url: / (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x000001E31EFB1B10>: Failed to resolve 'www.research.gla.ac.uk' ([Errno 11001] getaddrinfo failed)\"))\n",
      "{}\n",
      "\n",
      "--- Scraping https://www.bebob.de ---\n",
      "\n",
      "Error fetching https://www.bebob.de: HTTPSConnectionPool(host='www.bebob.de', port=443): Max retries exceeded with url: / (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x000001E31EFB1A10>, 'Connection to www.bebob.de timed out. (connect timeout=10)'))\n",
      "{}\n",
      "\n",
      "--- Scraping https://www.as.gov.qa ---\n",
      "\n",
      "Error fetching https://www.as.gov.qa: HTTPSConnectionPool(host='www.as.gov.qa', port=443): Max retries exceeded with url: / (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x000001E31EFB3F50>: Failed to resolve 'www.as.gov.qa' ([Errno 11001] getaddrinfo failed)\"))\n",
      "{}\n",
      "\n",
      "--- Scraping https://www.airjouletech.com ---\n",
      "\n",
      "{'emails': [], 'phones': [], 'fax': [], 'address': None}\n",
      "\n",
      "--- Scraping https://www.moser-konstruktion.de ---\n",
      "\n",
      "Error fetching https://www.moser-konstruktion.de: HTTPSConnectionPool(host='www.moser-konstruktion.de', port=443): Max retries exceeded with url: / (Caused by SSLError(SSLError(1, '[SSL: SSLV3_ALERT_HANDSHAKE_FAILURE] sslv3 alert handshake failure (_ssl.c:1002)')))\n",
      "{}\n",
      "\n",
      "--- Scraping https://www.ac.sce.ac.il ---\n",
      "\n",
      "Error fetching https://www.ac.sce.ac.il: HTTPSConnectionPool(host='www.ac.sce.ac.il', port=443): Max retries exceeded with url: / (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x000001E31EFBF190>: Failed to resolve 'www.ac.sce.ac.il' ([Errno 11001] getaddrinfo failed)\"))\n",
      "{}\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def normalize_phone(number: str) -> str:\n",
    "    number = number.strip()\n",
    "    number = re.sub(r\"[^\\d+]\", \"\", number)  # keep only digits and '+'\n",
    "\n",
    "    # Malaysia example\n",
    "    if number.startswith(\"603\"):\n",
    "        return \"+60 \" + number[2:]\n",
    "    if number.startswith(\"+60\"):\n",
    "        return number\n",
    "    return number\n",
    "\n",
    "def extract_address(text_lines):\n",
    "    ignore_keywords = [\"skip to content\", \"menu\", \"footer\", \"sitemap\", \"disclaimer\", \"policy\"]\n",
    "    address_keywords = r\"(Persiaran|Bandar|Selangor|Kajang|Malaysia)\"\n",
    "\n",
    "    for i, line in enumerate(text_lines):\n",
    "        line_lower = line.lower()\n",
    "        if any(k in line_lower for k in ignore_keywords):\n",
    "            continue  # skip UI/footer lines\n",
    "\n",
    "        if re.search(r\"\\d+\", line) and re.search(address_keywords, line, re.IGNORECASE):\n",
    "            # include next line if short and not ignored\n",
    "            candidate = line\n",
    "            if i + 1 < len(text_lines):\n",
    "                next_line = text_lines[i+1].strip()\n",
    "                if len(next_line) < 80 and not any(k in next_line.lower() for k in ignore_keywords):\n",
    "                    candidate += \" \" + next_line\n",
    "            return candidate.strip()\n",
    "    return None\n",
    "\n",
    "\n",
    "def scrape_contact_info(url):\n",
    "    try:\n",
    "        resp = requests.get(url, headers={\"User-Agent\": \"Mozilla/5.0\"}, timeout=10)\n",
    "        resp.raise_for_status()\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching {url}: {e}\")\n",
    "        return {}\n",
    "\n",
    "    soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "\n",
    "    # Replace <br> with newline to preserve line separation\n",
    "    for br in soup.find_all(\"br\"):\n",
    "        br.replace_with(\"\\n\")\n",
    "\n",
    "    # Extract visible text lines\n",
    "    text_lines = [line.strip() for line in soup.stripped_strings if line.strip()]\n",
    "\n",
    "    # ----------------- Extract emails -----------------\n",
    "    emails = set()\n",
    "    # from <a href=\"mailto:...\">\n",
    "    for a in soup.find_all(\"a\", href=True):\n",
    "        if a['href'].lower().startswith(\"mailto:\"):\n",
    "            emails.add(a['href'].split(\":\", 1)[1].strip())\n",
    "    # fallback: regex in text\n",
    "    for line in text_lines:\n",
    "        for e in re.findall(r\"[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,}\", line):\n",
    "            emails.add(e)\n",
    "\n",
    "    # ----------------- Extract phones -----------------\n",
    "    phones = set()\n",
    "    for a in soup.find_all(\"a\", href=True):\n",
    "        if a['href'].lower().startswith(\"tel:\"):\n",
    "            phones.add(normalize_phone(a['href'].split(\":\", 1)[1].strip()))\n",
    "    # fallback: regex in text\n",
    "    phone_pattern = r\"(?:Tel|Phone|Telefon)[:.]?\\s*([+0-9\\s\\-]{6,})\"\n",
    "    for line in text_lines:\n",
    "        match = re.search(phone_pattern, line, re.IGNORECASE)\n",
    "        if match:\n",
    "            phones.add(normalize_phone(match.group(1)))\n",
    "\n",
    "    # ----------------- Extract fax -----------------\n",
    "    fax_numbers = set()\n",
    "    fax_pattern = r\"(?:Fax)[:.]?\\s*([+0-9\\s\\-]{6,})\"\n",
    "    for line in text_lines:\n",
    "        match = re.search(fax_pattern, line, re.IGNORECASE)\n",
    "        if match:\n",
    "            fax_numbers.add(normalize_phone(match.group(1)))\n",
    "\n",
    "    # ----------------- Extract address -----------------\n",
    "    address_keywords = r\"(Persiaran|Bandar|Selangor|Kajang|Malaysia)\"\n",
    "    # Replace the previous address parsing with:\n",
    "    address = extract_address(text_lines)\n",
    "\n",
    "    return {\n",
    "        \"emails\": list(emails),\n",
    "        \"phones\": list(phones),\n",
    "        \"fax\": list(fax_numbers),\n",
    "        \"address\": address\n",
    "    }\n",
    "\n",
    "# -------------------------------\n",
    "# Example\n",
    "# -------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    websites = ['https://www.bathcanalcraft.co.uk',\n",
    "        \"https://www.kit.edu/\", \n",
    "        \"https://www.mpob.gov.my/\",\n",
    "        \"https://www.nseindia.com/\", 'https://www.research.gla.ac.uk', \n",
    "        'https://www.bebob.de', 'https://www.as.gov.qa', 'https://www.airjouletech.com',\n",
    "        'https://www.moser-konstruktion.de', 'https://www.ac.sce.ac.il'\n",
    "    ]\n",
    "\n",
    "    for url in websites:\n",
    "        print(f\"\\n--- Scraping {url} ---\\n\")\n",
    "        summary = scrape_contact_info(url)\n",
    "        print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "347a10c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Scraping address from https://www.bathcanalcraft.co.uk ---\n",
      "Address: NB ‘Alica-Lee‘ (in planning)\n",
      "\n",
      "--- Scraping address from https://www.kit.edu/ ---\n",
      "Address: Gebärdensprache\n",
      "\n",
      "--- Scraping address from https://www.mpob.gov.my/ ---\n",
      "Address: Malaysian Palm Oil Board – 6, Persiaran Institusi, Bandar Baru Bangi<br>43000 Kajang Selangor, Malaysia\n",
      "\n",
      "--- Scraping address from https://www.nseindia.com/ ---\n",
      "Address: 04-Sep-2025 14:46 IST\n",
      "\n",
      "--- Scraping address from https://www.research.gla.ac.uk ---\n",
      "Error fetching https://www.research.gla.ac.uk: HTTPSConnectionPool(host='www.research.gla.ac.uk', port=443): Max retries exceeded with url: / (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x000001E31EA7CB90>: Failed to resolve 'www.research.gla.ac.uk' ([Errno 11001] getaddrinfo failed)\"))\n",
      "Address: Not found\n",
      "\n",
      "--- Scraping address from https://www.bebob.de ---\n",
      "Error fetching https://www.bebob.de: HTTPSConnectionPool(host='www.bebob.de', port=443): Max retries exceeded with url: / (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x000001E31F16D5D0>, 'Connection to www.bebob.de timed out. (connect timeout=10)'))\n",
      "Address: Not found\n",
      "\n",
      "--- Scraping address from https://www.as.gov.qa ---\n",
      "Error fetching https://www.as.gov.qa: HTTPSConnectionPool(host='www.as.gov.qa', port=443): Max retries exceeded with url: / (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x000001E31F16F5D0>: Failed to resolve 'www.as.gov.qa' ([Errno 11001] getaddrinfo failed)\"))\n",
      "Address: Not found\n",
      "\n",
      "--- Scraping address from https://www.airjouletech.com ---\n",
      "Address: Applications\n",
      "\n",
      "--- Scraping address from https://www.moser-konstruktion.de ---\n",
      "Error fetching https://www.moser-konstruktion.de: HTTPSConnectionPool(host='www.moser-konstruktion.de', port=443): Max retries exceeded with url: / (Caused by SSLError(SSLError(1, '[SSL: SSLV3_ALERT_HANDSHAKE_FAILURE] sslv3 alert handshake failure (_ssl.c:1002)')))\n",
      "Address: Not found\n",
      "\n",
      "--- Scraping address from https://www.ac.sce.ac.il ---\n",
      "Error fetching https://www.ac.sce.ac.il: HTTPSConnectionPool(host='www.ac.sce.ac.il', port=443): Max retries exceeded with url: / (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x000001E31F16EA10>: Failed to resolve 'www.ac.sce.ac.il' ([Errno 11001] getaddrinfo failed)\"))\n",
      "Address: Not found\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "def extract_address_from_text(text_lines):\n",
    "    # Keywords commonly found in addresses\n",
    "    address_keywords = [\n",
    "        'street', 'strasse', 'road', 'rd', 'avenue', 'ave', 'lane', 'ln', 'boulevard', 'blvd',\n",
    "        'way', 'place', 'pl', 'square', 'sq', 'drive', 'dr', 'building', 'bldg', 'no.', 'nr.',\n",
    "        'suite', 'block', 'kaiserstraße', 'jalan', 'city', 'state', 'zip', 'postcode', 'country'\n",
    "    ]\n",
    "    postal_code_pattern = re.compile(r'\\b\\d{4,6}\\b')\n",
    "\n",
    "    for line in text_lines:\n",
    "        line_lower = line.lower()\n",
    "        if any(k in line_lower for k in address_keywords) or postal_code_pattern.search(line):\n",
    "            return line.strip()\n",
    "    return None\n",
    "\n",
    "def scrape_address(url):\n",
    "    try:\n",
    "        resp = requests.get(url, headers={\"User-Agent\": \"Mozilla/5.0\"}, timeout=10)\n",
    "        resp.raise_for_status()\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "    soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "    # Replace <br> with newline to preserve line separation\n",
    "    for br in soup.find_all(\"br\"):\n",
    "        br.replace_with(\"\\n\")\n",
    "\n",
    "    # Extract visible text lines\n",
    "    text_lines = [line.strip() for line in soup.stripped_strings if line.strip()]\n",
    "\n",
    "    address = extract_address_from_text(text_lines)\n",
    "    return address\n",
    "\n",
    "# Example usage\n",
    "websites = [\n",
    "    'https://www.bathcanalcraft.co.uk',\n",
    "    'https://www.kit.edu/',\n",
    "    'https://www.mpob.gov.my/',\n",
    "    'https://www.nseindia.com/',\n",
    "    'https://www.research.gla.ac.uk',\n",
    "    'https://www.bebob.de',\n",
    "    'https://www.as.gov.qa',\n",
    "    'https://www.airjouletech.com',\n",
    "    'https://www.moser-konstruktion.de',\n",
    "    'https://www.ac.sce.ac.il'\n",
    "]\n",
    "\n",
    "for url in websites:\n",
    "    print(f\"\\n--- Scraping address from {url} ---\")\n",
    "    address = scrape_address(url)\n",
    "    print(\"Address:\", address if address else \"Not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ddc263ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Scraping full address from https://www.bathcanalcraft.co.uk ---\n",
      "Full Address: Not found\n",
      "\n",
      "--- Scraping full address from https://www.kit.edu/ ---\n",
      "Full Address: KIT. Ort der Zukunft. Seit 1825.\n",
      "Full Address: Save the Date: KIT Science Week 2025 startet im Oktober.\n",
      "Full Address: 04. September 2025 13:30 - 16:30 Fortbildungszentrum für Technik und Umwelt (FTU)\n",
      "Full Address: Für eine Million Jahre muss der Müll des Atomzeitalters, also abgebrannte Brennstäbe aus Kernkraftwerken und andere radioaktive Materialien, in Deutschland sicher verwahrt werden. Die hochradioaktiven Abfälle sollen tief unter der Erde gelagert werden – sicher verschlossen und unzugänglich für zukünftige Generationen. Aber wer entscheidet eigentlich, wo und wie so ein Lager entsteht?\n",
      "\n",
      " \n",
      "\n",
      "Die Ausstellung „Wer entscheidet denn sowas? Endlager und Demokratie“ des Instituts für Technikfolgenabschätzung und Systemanalyse (ITAS) des KIT lädt dazu ein, sich mit dieser Frage auseinanderzusetzen. Kooperationspartner sind das Institut für Nukleare Entsorgung des KIT, die Kerntechnischen Entsorgung Karlsruhe GmbH (KTE) und das KIT-Zentrum Mensch und Technik.\n",
      "\n",
      " \n",
      "\n",
      "Zur Eröffnung lädt das ITAS am Donnerstag, 4. September 2025, um 18 Uhr zu einer Diskussionsveranstaltung ein. \n",
      "\n",
      " \n",
      "\n",
      "Für diese Veranstaltung ist eine Anmeldung erforderlich. Siehe Link unten!\n",
      "Full Address: Werde selbst zur Forscherin oder zum Forscher!\n",
      "\n",
      "Was beeinflusst unser Klima? Wie können wir Veränderungen in unserer Umwelt erkennen?\n",
      "\n",
      "Im Science Camp: Klima & Umwelt tauchst du mitten in die spannende Welt der Forschung ein.\n",
      "\n",
      "Erforsche mit uns die Atmosphäre, den Wasserkreislauf, Ökosysteme und Böden. Wir untersuchen, welchen Einfluss der Mensch und die Stadt auf das Klima haben ? mit Experimenten, Recherchen und eigenen Datenerhebungen.\n",
      "\n",
      "- Das erwartet dich:\n",
      "- Keine Vorkenntnisse nötig - einfach mitmachen!\n",
      "- Forsche an spannenden Fragestellungen\n",
      "- Bekomme von Klima- und Wetterforscherinnen exklusive Einblicke in aktuelle Forschungsthemen\n",
      "- Beim gemeinsamen Bouldern und Pizzaessen gibt es genug Zeit zum Entspannen und Austauschen\n",
      "\n",
      "Das Camp beginnt am 8. September 2025 und endet am 12. September.\n",
      "\n",
      "Für Kinder und Jugendliche der Klassenstufen 7-10.\n",
      "\n",
      "Kostenpflichtige Anmeldung: 100 €\n",
      "Finanzielle Unterstützung bei Bedarf möglich. Alle weiteren Informationen findest du auf unserer Website!\n",
      "Full Address: 2025 Science Camp Informatik vom 08.09.-12.09.2025 Wir räumen auf mit Vorurteilen und wollen im Rahmen des Science Camps zeigen, dass Informatik viele Facetten hat und genauso Mädchen wie Jungs begeistern kann. \n",
      "\n",
      "Egal ob in der Medizin, im Film, in der Automobilindustrie oder in der Finanzwelt: Informatik hilft heutzutage in fast allen Lebensbereichen, Prozesse zu optimieren und Informationen zu sammeln und darzustellen. Die Arbeit von Informatiker:innen spielt sich dabei aber nicht ausschließlich alleine vor dem Computer ab, vielmehr sind die Zusammenarbeit im Team und eine stetige und gute Kommunikation für ein erfolgreiches Informatikprojekt entscheidend.\n",
      "\n",
      "Habt ihr Lust, das Fach Informatik in fünf Workshop-Tagen gemeinsam mit anderen Mädchen kennenzulernen, euch mit Informatikthemen praktisch zu beschäftigen und mit uns eure ersten Schritte als Coderinnen zu gehen?\n",
      "\n",
      "Dann ist das Science Camp Informatik genau das Richtige für Euch!\n",
      "\n",
      "Nach einer Einführung HTML erstellt jede von euch eine eigene Website, die mithilfe von CSS und JavaScript ganz nach Belieben gestaltet werden kann. Daneben gibt‘s auch Einblicke in andere interessante Bereiche der Informatik und ihr lernt andere Mädchen und Frauen kennen, die Informatik studieren oder bereits im Informatikbereich arbeiten und könnt so von deren Erlebnissen und Erfahrungen profitieren.\n",
      "\n",
      "--- Scraping full address from https://www.mpob.gov.my/ ---\n",
      "Full Address: Malaysian Palm Oil Board – 6, Persiaran Institusi, Bandar Baru Bangi<br>43000 Kajang Selangor, Malaysia\n",
      "Full Address: Nutrition Satellite Symposium in conjunction with PIPOC 2025 ISOPB - Advancing The Next Generation of Oil Palm Planting Materials & Annual General Meeting MPOB International Palm Oil Congress and Exhibition (PIPOC) 2025 International Conference on Oil Palm Plant Protection (ICOPP) 2025 Palm Oil Economic Review and Outlook Seminar R&O 2026\n",
      "Full Address: Slider Malaysian Standard 1.8.2025\n",
      "Full Address: © 2025 Malaysian Palm Oil Board\n",
      "\n",
      "--- Scraping full address from https://www.nseindia.com/ ---\n",
      "Full Address: Not found\n",
      "\n",
      "--- Scraping full address from https://www.research.gla.ac.uk ---\n",
      "Error fetching https://www.research.gla.ac.uk: HTTPSConnectionPool(host='www.research.gla.ac.uk', port=443): Max retries exceeded with url: / (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x000001E31F362790>: Failed to resolve 'www.research.gla.ac.uk' ([Errno 11001] getaddrinfo failed)\"))\n",
      "Full Address: Not found\n",
      "\n",
      "--- Scraping full address from https://www.bebob.de ---\n",
      "Error fetching https://www.bebob.de: HTTPSConnectionPool(host='www.bebob.de', port=443): Max retries exceeded with url: / (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x000001E31F2E9D10>, 'Connection to www.bebob.de timed out. (connect timeout=10)'))\n",
      "Full Address: Not found\n",
      "\n",
      "--- Scraping full address from https://www.as.gov.qa ---\n",
      "Error fetching https://www.as.gov.qa: HTTPSConnectionPool(host='www.as.gov.qa', port=443): Max retries exceeded with url: / (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x000001E31F2EBCD0>: Failed to resolve 'www.as.gov.qa' ([Errno 11001] getaddrinfo failed)\"))\n",
      "Full Address: Not found\n",
      "\n",
      "--- Scraping full address from https://www.airjouletech.com ---\n",
      "Full Address: AirJoule Technologies Announces Second Quarter 2025 Results AirJoule Technologies to Showcase its Groundbreaking Platform for “Waste Heat to Water” Project in Texas, Unleashing the Power of Water from Air\n",
      "\n",
      "--- Scraping full address from https://www.moser-konstruktion.de ---\n",
      "Error fetching https://www.moser-konstruktion.de: HTTPSConnectionPool(host='www.moser-konstruktion.de', port=443): Max retries exceeded with url: / (Caused by SSLError(SSLError(1, '[SSL: SSLV3_ALERT_HANDSHAKE_FAILURE] sslv3 alert handshake failure (_ssl.c:1002)')))\n",
      "Full Address: Not found\n",
      "\n",
      "--- Scraping full address from https://www.ac.sce.ac.il ---\n",
      "Error fetching https://www.ac.sce.ac.il: HTTPSConnectionPool(host='www.ac.sce.ac.il', port=443): Max retries exceeded with url: / (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x000001E31F2F08D0>: Failed to resolve 'www.ac.sce.ac.il' ([Errno 11001] getaddrinfo failed)\"))\n",
      "Full Address: Not found\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "def extract_full_address(text_lines):\n",
    "    # Keywords commonly found in addresses\n",
    "    address_keywords = [\n",
    "        'street', 'strasse', 'road', 'rd', 'avenue', 'ave', 'lane', 'ln', 'boulevard', 'blvd',\n",
    "        'way', 'place', 'pl', 'square', 'sq', 'drive', 'dr', 'building', 'bldg', 'no.', 'nr.',\n",
    "        'suite', 'block', 'kaiserstraße', 'jalan', 'city', 'state', 'zip', 'postcode', 'country',\n",
    "        'post', 'mail', 'address', 'p.o.', 'po box', 'plz', 'ort', 'hausnummer', 'straße', 'str.'\n",
    "    ]\n",
    "    postal_code_pattern = re.compile(r'\\b\\d{4,6}\\b')\n",
    "\n",
    "    addresses = []\n",
    "    block = []\n",
    "    for line in text_lines:\n",
    "        line_lower = line.lower()\n",
    "        # Check if line is part of an address block\n",
    "        if any(k in line_lower for k in address_keywords) or postal_code_pattern.search(line):\n",
    "            block.append(line.strip())\n",
    "        else:\n",
    "            if block:\n",
    "                # If block has at least street, city, and postal code, consider it a full address\n",
    "                block_text = ' '.join(block)\n",
    "                if postal_code_pattern.search(block_text) and any(k in block_text.lower() for k in address_keywords):\n",
    "                    addresses.append(block_text)\n",
    "                block = []\n",
    "    # Catch any trailing block\n",
    "    if block:\n",
    "        block_text = ' '.join(block)\n",
    "        if postal_code_pattern.search(block_text) and any(k in block_text.lower() for k in address_keywords):\n",
    "            addresses.append(block_text)\n",
    "    return addresses if addresses else None\n",
    "\n",
    "def scrape_full_address(url):\n",
    "    try:\n",
    "        resp = requests.get(url, headers={\"User-Agent\": \"Mozilla/5.0\"}, timeout=10)\n",
    "        resp.raise_for_status()\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "    soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "    for br in soup.find_all(\"br\"):\n",
    "        br.replace_with(\"\\n\")\n",
    "\n",
    "    text_lines = [line.strip() for line in soup.stripped_strings if line.strip()]\n",
    "\n",
    "    addresses = extract_full_address(text_lines)\n",
    "    return addresses\n",
    "\n",
    "# Example usage\n",
    "websites = [\n",
    "    'https://www.bathcanalcraft.co.uk',\n",
    "    'https://www.kit.edu/',\n",
    "    'https://www.mpob.gov.my/',\n",
    "    'https://www.nseindia.com/',\n",
    "    'https://www.research.gla.ac.uk',\n",
    "    'https://www.bebob.de',\n",
    "    'https://www.as.gov.qa',\n",
    "    'https://www.airjouletech.com',\n",
    "    'https://www.moser-konstruktion.de',\n",
    "    'https://www.ac.sce.ac.il'\n",
    "]\n",
    "\n",
    "for url in websites:\n",
    "    print(f\"\\n--- Scraping full address from {url} ---\")\n",
    "    addresses = scrape_full_address(url)\n",
    "    if addresses:\n",
    "        for addr in addresses:\n",
    "            print(\"Full Address:\", addr)\n",
    "    else:\n",
    "        print(\"Full Address: Not found\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
